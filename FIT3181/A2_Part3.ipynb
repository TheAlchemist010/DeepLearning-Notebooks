{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheAlchemist010/DeepLearning-Notebooks/blob/main/FIT3181/A2_Part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"#0b486b\">  FIT3181: Deep Learning (2024) - Assignment 2 (Transformers)</font>\n",
        "***\n",
        "*CE/Lecturer (Clayton):*  **Dr Trung Le** | trunglm@monash.edu <br/>\n",
        "*Lecturer (Clayton):* **Prof Dinh Phung** | dinh.phung@monash.edu <br/>\n",
        "*Lecturer (Malaysia):*  **Dr Arghya Pal** | arghya.pal@monash.edu <br/>\n",
        "*Lecturer (Malaysia):*  **Dr Lim Chern Hong** | lim.chernhong@monash.edu <br/>  <br/>\n",
        "*Head Tutor 3181:*  **Miss Vy Vo** |  \\[v.vo@monash.edu \\] <br/>\n",
        "*Head Tutor 5215:*  **Dr Van Nguyen** |  \\[van.nguyen1@monash.edu \\]\n",
        "\n",
        "<br/> <br/>\n",
        "Faculty of Information Technology, Monash University, Australia\n",
        "***"
      ],
      "metadata": {
        "id": "qEHyseHxA8q4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"#0b486b\">  Student Information</font>\n",
        "***\n",
        "Surname: **Gallagher**  <br/>\n",
        "Firstname: **Daniel**    <br/>\n",
        "Student ID: **33094969**    <br/>\n",
        "Email: **dgal0013@student.monash.edu**    <br/>\n",
        "Your tutorial time: **Monday 4pm**    <br/>\n",
        "***"
      ],
      "metadata": {
        "id": "QqMi8gdDBD1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"0b486b\">Assignment 2 â€“ Deep Learning for Sequential Data</font>\n",
        "### Due: <font color=\"red\">11:55pm Sunday, 27 October 2024</font> (FIT3181)\n",
        "\n",
        "#### <font color=\"red\">Important note:</font> This is an **individual** assignment. It contributes **15%** to your final mark. Read the assignment instructions carefully."
      ],
      "metadata": {
        "id": "BLGpdQp2BPOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"#0b486b\">Assignment 2's Organization</font>\n",
        "This assignment 2 has two (2) sections:\n",
        "- Section 1: Fundamentals of RNNs (10 marks).\n",
        "- Section 2: Deep Learning for Sequential Data (90 marks). This section is further divided into 4 parts.\n",
        "\n",
        "The assignment 2 is organized in three (3) notebooks.\n",
        "- Notebook 1 ([link](https://colab.research.google.com/drive/1Rm2wWOJCjilpVf4T6RJZFtkjRULuTo0g?usp=sharing)) [Total: 30 marks] includes Section 1 as well as Part 1 and Part 2 of Section 2.\n",
        "- Notebook 2 ([link](https://colab.research.google.com/drive/19-WnvLH24yUZ_eih3_8P_Fjn8cM8X2Gz?usp=sharing)) [Total: 40 marks] includes Part 3 of Section 2.\n",
        "- Notebook 3 (this notebook) [Total: 30 marks] includes Part 4 of Section 2.\n"
      ],
      "metadata": {
        "id": "PF8vqRzTCEsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"#0b486b\">What to submit</font>\n",
        "\n",
        "This assignment is to be completed individually and submitted to Moodle unit site. **By the due date, you are required to submit one  <font color=\"red; font-weight:bold\">single zip file, named xxx_assignment02_solution.zip</font> where `xxx` is your student ID, to the corresponding Assignment (Dropbox) in Moodle**. You can use Google Colab to do Assignment 2 but you need to save it to an `*.ipynb` file to submit to the unit Moodle.\n",
        "\n",
        "**More importantly, if you use Google Colab to do this assignment, you need to first make a copy of this notebook on your Google drive**."
      ],
      "metadata": {
        "id": "UbY5tyqK37Iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***For example, if your student ID is <font color=\"red; font-weight:bold\">12356</font>, then gather all of your assignment solutions to a folder, create a zip file named <font color=\"red; font-weight:bold\">123456_assignment02_solution.zip</font> and submit this file.***"
      ],
      "metadata": {
        "id": "cg1MdTwq37I6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Within this zip folder, you **must** submit the following files <u>for each part</u>:\n",
        "1.\t**`FIT3181_DeepLearning_Assignment2_Official[Main].ipynb`**:  this is your Python notebook solution source file.\n",
        "1.\t**`FIT3181_DeepLearning_Assignment2_Official[Main].html`**: this is the output of your Python notebook solution *exported* in HTML format.\n",
        "1. **`FIT3181_DeepLearning_Assignment2_Official[RNNs].ipynb`**\n",
        "1. **`FIT3181_DeepLearning_Assignment2_Official[RNNs].html`**\n",
        "1. **`FIT3181_DeepLearning_Assignment2_Official[Transformers].ipynb`**\n",
        "1. **`FIT3181_DeepLearning_Assignment2_Official[Transformers].html`**\n",
        "1.\tAny **extra files or folder** needed to complete your assignment (e.g., images used in your answers).\n",
        "\n"
      ],
      "metadata": {
        "id": "prEJdUhB37I6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2: Deep Learning for Sequential Data"
      ],
      "metadata": {
        "id": "rAVPM0BnTdd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"#0b486b\">Set random seeds</font>"
      ],
      "metadata": {
        "id": "FAgUSME4TsCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to install the package datasets for creating BERT datasets."
      ],
      "metadata": {
        "id": "5DaYHAIv4poj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "o7ik_46Y4vDo",
        "outputId": "3bb98093-d309-4229-e70f-2186c2560a7b",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:09:37.283928Z",
          "iopub.execute_input": "2024-10-29T01:09:37.284332Z",
          "iopub.status.idle": "2024-10-29T01:09:49.563958Z",
          "shell.execute_reply.started": "2024-10-29T01:09:37.284294Z",
          "shell.execute_reply": "2024-10-29T01:09:49.562785Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start with importing PyTorch and NumPy and setting random seeds for PyTorch and NumPy. You can use any seeds you prefer."
      ],
      "metadata": {
        "id": "g6b0SM034sf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import random\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import BertTokenizer\n",
        "import os\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')"
      ],
      "metadata": {
        "id": "O7XWUry0JXCc",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:09:49.566107Z",
          "iopub.execute_input": "2024-10-29T01:09:49.566466Z",
          "iopub.status.idle": "2024-10-29T01:09:50.043803Z",
          "shell.execute_reply.started": "2024-10-29T01:09:49.566431Z",
          "shell.execute_reply": "2024-10-29T01:09:50.043019Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_all(seed=1029):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "seed_all(seed=1234)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "6ZoWqunmUY7L",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:09:50.044872Z",
          "iopub.execute_input": "2024-10-29T01:09:50.045606Z",
          "iopub.status.idle": "2024-10-29T01:09:50.113698Z",
          "shell.execute_reply.started": "2024-10-29T01:09:50.045571Z",
          "shell.execute_reply": "2024-10-29T01:09:50.112943Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"#0b486b\">Download and preprocess the data</font>\n",
        "\n",
        "<div style=\"text-align: right\"><font color=\"red; font-weight:bold\"><span></div>"
      ],
      "metadata": {
        "id": "6VU1jS6SUl8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset we use for this assignment is a question classification dataset for which the training set consists of $5,500$ questions belonging to 6 coarse question categories including:\n",
        "- abbreviation (ABBR),\n",
        "- entity (ENTY),\n",
        "- description (DESC),\n",
        "- human (HUM),\n",
        "- location (LOC) and\n",
        "- numeric (NUM).\n",
        "\n",
        "In this assignment, we will utilize a subset of this dataset, containing $2,000$ questions for training and validation. We will use 80% of those 2000 questions for trainning and the rest for validation.\n"
      ],
      "metadata": {
        "id": "wQEzWmZjUulL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing data is a crucial initial step in any machine learning or deep learning project. The *TextDataManager* class simplifies the process by providing functionalities to download and preprocess data specifically designed for the subsequent questions in this assignment. It is highly recommended to gain a comprehensive understanding of the class's functionality by **carefully reading** the content provided in the *TextDataManager* class before proceeding to answer the questions."
      ],
      "metadata": {
        "id": "zOd49RTpUxxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataManager:\n",
        "    \"\"\"\n",
        "    This class manages and preprocesses a simple text dataset for a sentence classification task.\n",
        "\n",
        "    Attributes:\n",
        "        verbose (bool): Controls verbosity for printing information during data processing.\n",
        "        max_sentence_len (int): The maximum length of a sentence in the dataset.\n",
        "        str_questions (list): A list to store the string representations of the questions in the dataset.\n",
        "        str_labels (list): A list to store the string representations of the labels in the dataset.\n",
        "        numeral_labels (list): A list to store the numerical representations of the labels in the dataset.\n",
        "        maxlen (int): Maximum length for padding sequences. Sequences longer than this length will be truncated,\n",
        "            and sequences shorter than this length will be padded with zeros. Defaults to 50.\n",
        "        numeral_data (list): A list to store the numerical representations of the questions in the dataset.\n",
        "        random_state (int): Seed value for random number generation to ensure reproducibility.\n",
        "            Set this value to a specific integer to reproduce the same random sequence every time. Defaults to 6789.\n",
        "        random (np.random.RandomState): Random number generator object initialized with the given random_state.\n",
        "            It is used for various random operations in the class.\n",
        "\n",
        "    Methods:\n",
        "        maybe_download(dir_name, file_name, url, verbose=True):\n",
        "            Downloads a file from a given URL if it does not exist in the specified directory.\n",
        "            The directory and file are created if they do not exist.\n",
        "\n",
        "        read_data(dir_name, file_names):\n",
        "            Reads data from files in a directory, preprocesses it, and computes the maximum sentence length.\n",
        "            Each file is expected to contain rows in the format \"<label>:<question>\".\n",
        "            The labels and questions are stored as string representations.\n",
        "\n",
        "        manipulate_data():\n",
        "            Performs data manipulation by tokenizing, numericalizing, and padding the text data.\n",
        "            The questions are tokenized and converted into numerical sequences using a tokenizer.\n",
        "            The sequences are padded or truncated to the maximum sequence length.\n",
        "\n",
        "        train_valid_test_split(train_ratio=0.9):\n",
        "            Splits the data into training, validation, and test sets based on a given ratio.\n",
        "            The data is randomly shuffled, and the specified ratio is used to determine the size of the training set.\n",
        "            The string questions, numerical data, and numerical labels are split accordingly.\n",
        "            TensorFlow `Dataset` objects are created for the training and validation sets.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, verbose=True, random_state=6789):\n",
        "        self.verbose = verbose\n",
        "        self.max_sentence_len = 0\n",
        "        self.str_questions = list()\n",
        "        self.str_labels = list()\n",
        "        self.numeral_labels = list()\n",
        "        self.numeral_data = list()\n",
        "        self.random_state = random_state\n",
        "        self.random = np.random.RandomState(random_state)\n",
        "\n",
        "    @staticmethod\n",
        "    def maybe_download(dir_name, file_name, url, verbose=True):\n",
        "        if not os.path.exists(dir_name):\n",
        "            os.mkdir(dir_name)\n",
        "        if not os.path.exists(os.path.join(dir_name, file_name)):\n",
        "            urlretrieve(url + file_name, os.path.join(dir_name, file_name))\n",
        "        if verbose:\n",
        "            print(\"Downloaded successfully {}\".format(file_name))\n",
        "\n",
        "    def read_data(self, dir_name, file_names):\n",
        "        self.str_questions = list()\n",
        "        self.str_labels = list()\n",
        "        for file_name in file_names:\n",
        "            file_path= os.path.join(dir_name, file_name)\n",
        "            with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
        "                for row in f:\n",
        "                    row_str = row.split(\":\")\n",
        "                    label, question = row_str[0], row_str[1]\n",
        "                    question = question.lower()\n",
        "                    self.str_labels.append(label)\n",
        "                    self.str_questions.append(question[0:-1])\n",
        "                    if self.max_sentence_len < len(self.str_questions[-1]):\n",
        "                        self.max_sentence_len = len(self.str_questions[-1])\n",
        "\n",
        "        # turns labels into numbers\n",
        "        le = preprocessing.LabelEncoder()\n",
        "        le.fit(self.str_labels)\n",
        "        self.numeral_labels = np.array(le.transform(self.str_labels))\n",
        "        self.str_classes = le.classes_\n",
        "        self.num_classes = len(self.str_classes)\n",
        "        if self.verbose:\n",
        "            print(\"\\nSample questions and corresponding labels... \\n\")\n",
        "            print(self.str_questions[0:5])\n",
        "            print(self.str_labels[0:5])\n",
        "\n",
        "    def manipulate_data(self):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        vocab = self.tokenizer.get_vocab()\n",
        "        self.word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "        self.idx2word = {i:w for w,i in self.word2idx.items()}\n",
        "        self.vocab_size = len(self.word2idx)\n",
        "\n",
        "        token_ids = []\n",
        "        num_seqs = []\n",
        "        for text in self.str_questions:  # iterate over the list of text\n",
        "          text_seqs = self.tokenizer.tokenize(str(text))  # tokenize each text individually\n",
        "          # Convert tokens to IDs\n",
        "          token_ids = self.tokenizer.convert_tokens_to_ids(text_seqs)\n",
        "          # Convert token IDs to a tensor of indices using your word2idx mapping\n",
        "          seq_tensor = torch.LongTensor(token_ids)\n",
        "          num_seqs.append(seq_tensor)  # append the tensor for each sequence\n",
        "\n",
        "        # Pad the sequences and create a tensor\n",
        "        if num_seqs:\n",
        "          self.numeral_data = pad_sequence(num_seqs, batch_first=True)  # Pads to max length of the sequences\n",
        "          self.num_sentences, self.max_seq_len = self.numeral_data.shape\n",
        "\n",
        "    def train_valid_test_split(self, train_ratio=0.8, test_ratio = 0.1):\n",
        "        train_size = int(self.num_sentences*train_ratio) +1\n",
        "        test_size = int(self.num_sentences*test_ratio) +1\n",
        "        valid_size = self.num_sentences - (train_size + test_size)\n",
        "        data_indices = list(range(self.num_sentences))\n",
        "        random.shuffle(data_indices)\n",
        "        self.train_str_questions = [self.str_questions[i] for i in data_indices[:train_size]]\n",
        "        self.train_numeral_labels = self.numeral_labels[data_indices[:train_size]]\n",
        "        train_set_data = self.numeral_data[data_indices[:train_size]]\n",
        "        train_set_labels = self.numeral_labels[data_indices[:train_size]]\n",
        "        train_set_labels = torch.from_numpy(train_set_labels)\n",
        "        train_set = torch.utils.data.TensorDataset(train_set_data, train_set_labels)\n",
        "        self.test_str_questions = [self.str_questions[i] for i in data_indices[-test_size:]]\n",
        "        self.test_numeral_labels = self.numeral_labels[data_indices[-test_size:]]\n",
        "        test_set_data = self.numeral_data[data_indices[-test_size:]]\n",
        "        test_set_labels = self.numeral_labels[data_indices[-test_size:]]\n",
        "        test_set_labels = torch.from_numpy(test_set_labels)\n",
        "        test_set = torch.utils.data.TensorDataset(test_set_data, test_set_labels)\n",
        "        self.valid_str_questions = [self.str_questions[i] for i in data_indices[train_size:-test_size]]\n",
        "        self.valid_numeral_labels = self.numeral_labels[data_indices[train_size:-test_size]]\n",
        "        valid_set_data = self.numeral_data[data_indices[train_size:-test_size]]\n",
        "        valid_set_labels = self.numeral_labels[data_indices[train_size:-test_size]]\n",
        "        valid_set_labels = torch.from_numpy(valid_set_labels)\n",
        "        valid_set = torch.utils.data.TensorDataset(valid_set_data, valid_set_labels)\n",
        "        self.train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "        self.test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
        "        self.valid_loader = DataLoader(valid_set, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "_C2fuJNzUhha",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:09:50.115514Z",
          "iopub.execute_input": "2024-10-29T01:09:50.115811Z",
          "iopub.status.idle": "2024-10-29T01:09:50.143576Z",
          "shell.execute_reply.started": "2024-10-29T01:09:50.115779Z",
          "shell.execute_reply": "2024-10-29T01:09:50.142708Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Loading data...')\n",
        "DataManager.maybe_download(\"data\", \"train_2000.label\", \"http://cogcomp.org/Data/QA/QC/\")\n",
        "\n",
        "dm = DataManager()\n",
        "dm.read_data(\"data/\", [\"train_2000.label\"])"
      ],
      "metadata": {
        "id": "3npdESj6Vb_t",
        "outputId": "402ffc6e-22f6-4745-e080-a88aa0647fc3",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:09:52.814506Z",
          "iopub.execute_input": "2024-10-29T01:09:52.814812Z",
          "iopub.status.idle": "2024-10-29T01:09:52.826341Z",
          "shell.execute_reply.started": "2024-10-29T01:09:52.814781Z",
          "shell.execute_reply": "2024-10-29T01:09:52.825540Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Loading data...\nDownloaded successfully train_2000.label\n\nSample questions and corresponding labels... \n\n['manner how did serfdom develop in and then leave russia ?', 'cremat what films featured the character popeye doyle ?', \"manner how can i find a list of celebrities ' real names ?\", 'animal what fowl grabs the spotlight after the chinese year of the monkey ?', 'exp what is the full form of .com ?']\n['DESC', 'ENTY', 'DESC', 'ENTY', 'ABBR']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dm.manipulate_data()\n",
        "dm.train_valid_test_split(train_ratio=0.8, test_ratio = 0.1)"
      ],
      "metadata": {
        "id": "EgrYZPmyVj60",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:09:55.388943Z",
          "iopub.execute_input": "2024-10-29T01:09:55.389599Z",
          "iopub.status.idle": "2024-10-29T01:09:56.474332Z",
          "shell.execute_reply.started": "2024-10-29T01:09:55.389556Z",
          "shell.execute_reply": "2024-10-29T01:09:56.473342Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dm.train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "bH-U0sUMVnW-",
        "outputId": "a6ad4b69-3dc0-4668-a47f-baeff927d928",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:09:56.916004Z",
          "iopub.execute_input": "2024-10-29T01:09:56.916390Z",
          "iopub.status.idle": "2024-10-29T01:09:56.941685Z",
          "shell.execute_reply.started": "2024-10-29T01:09:56.916351Z",
          "shell.execute_reply": "2024-10-29T01:09:56.940891Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "torch.Size([64, 36]) torch.Size([64])\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now declare the `BaseTrainer` class, which will be used later to train the subsequent deep learning models for text data."
      ],
      "metadata": {
        "id": "lPPrm2_FHj-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class BaseTrainer:\n",
        "    def __init__(self, model, criterion, optimizer, train_loader, val_loader):\n",
        "        self.model = model\n",
        "        self.criterion = criterion  #the loss function\n",
        "        self.optimizer = optimizer  #the optimizer\n",
        "        self.train_loader = train_loader  #the train loader\n",
        "        self.val_loader = val_loader  #the valid loader\n",
        "\n",
        "    #the function to train the model in many epochs\n",
        "    def fit(self, num_epochs):\n",
        "        self.num_batches = len(self.train_loader)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "            train_loss, train_accuracy = self.train_one_epoch()\n",
        "            val_loss, val_accuracy = self.validate_one_epoch()\n",
        "            print(\n",
        "                f'{self.num_batches}/{self.num_batches} - train_loss: {train_loss:.4f} - train_accuracy: {train_accuracy*100:.4f}% \\\n",
        "                - val_loss: {val_loss:.4f} - val_accuracy: {val_accuracy*100:.4f}%')\n",
        "\n",
        "    #train in one epoch, return the train_acc, train_loss\n",
        "    def train_one_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        train_accuracy = correct / total\n",
        "        train_loss = running_loss / self.num_batches\n",
        "        return train_loss, train_accuracy\n",
        "\n",
        "    #evaluate on a loader and return the loss and accuracy\n",
        "    def evaluate(self, loader):\n",
        "        self.model.eval()\n",
        "        loss, correct, total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for data in loader:\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = correct / total\n",
        "        loss = loss / len(self.val_loader)\n",
        "        return loss, accuracy\n",
        "\n",
        "    #return the val_acc, val_loss, be called at the end of each epoch\n",
        "    def validate_one_epoch(self):\n",
        "      val_loss, val_accuracy = self.evaluate(self.val_loader)\n",
        "      return val_loss, val_accuracy"
      ],
      "metadata": {
        "id": "yXlNQvGn7OEb",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:09:58.250412Z",
          "iopub.execute_input": "2024-10-29T01:09:58.251122Z",
          "iopub.status.idle": "2024-10-29T01:09:58.265850Z",
          "shell.execute_reply.started": "2024-10-29T01:09:58.251082Z",
          "shell.execute_reply": "2024-10-29T01:09:58.265020Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"#0b486b\">Part 4: Transformer-based models for sequence modeling and neural embedding</font>\n",
        "\n",
        "<div style=\"text-align: right\"><font color=\"red; font-weight:bold\">[Total marks for this part: 30 marks]<span></div>"
      ],
      "metadata": {
        "id": "sPvLRNDfoSq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"red\">**Question 4.1**</font>\n",
        "\n",
        "**Implement the multi-head attention module of the Transformer for the text classification problem. The provided code is from our tutorial. In this part, we only use the output of the Transformer encoder for the classification task. For further information on the Transformer model, refer to [this paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).**\n",
        "\n",
        "<div style=\"text-align: right\"><font color=\"red; font-weight:bold\">[Total marks for this part: 10 marks]<span></div>\n"
      ],
      "metadata": {
        "id": "QOoskR7Ko6Iv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the code of `MultiHeadSelfAttention`, `PositionWiseFeedForward`, `PositionalEncoding`, and `EncoderLayer`."
      ],
      "metadata": {
        "id": "LUnK0WBspLDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        # Initialize dimensions\n",
        "        self.d_model = d_model # Model's dimension\n",
        "        self.num_heads = num_heads # Number of attention heads\n",
        "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
        "\n",
        "        # Linear layers for transforming inputs\n",
        "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
        "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
        "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
        "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V):\n",
        "        # Calculate attention scores\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
        "        #if mask is not None:\n",
        "            #attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Softmax is applied to obtain attention probabilities\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # Multiply by values to obtain the final output\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Reshape the input to have num_heads for multi-head attention\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # Combine the multiple heads back to original shape\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V):\n",
        "        # Apply linear transformations and split heads\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        # Perform scaled dot-product attention\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "        # Combine heads and apply output transformation\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output"
      ],
      "metadata": {
        "id": "PERuLdjTZpAl",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:10:01.803821Z",
          "iopub.execute_input": "2024-10-29T01:10:01.804678Z",
          "iopub.status.idle": "2024-10-29T01:10:01.816406Z",
          "shell.execute_reply.started": "2024-10-29T01:10:01.804627Z",
          "shell.execute_reply": "2024-10-29T01:10:01.815539Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "J4MZuO59pR0T",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:10:03.806897Z",
          "iopub.execute_input": "2024-10-29T01:10:03.807834Z",
          "iopub.status.idle": "2024-10-29T01:10:03.813420Z",
          "shell.execute_reply.started": "2024-10-29T01:10:03.807791Z",
          "shell.execute_reply": "2024-10-29T01:10:03.812473Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "qq15ROA9pV3N",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:10:04.873266Z",
          "iopub.execute_input": "2024-10-29T01:10:04.874026Z",
          "iopub.status.idle": "2024-10-29T01:10:04.882555Z",
          "shell.execute_reply.started": "2024-10-29T01:10:04.873982Z",
          "shell.execute_reply": "2024-10-29T01:10:04.881521Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output = self.self_attn(x, x, x)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "HKaj3paKqTmG",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:10:06.970140Z",
          "iopub.execute_input": "2024-10-29T01:10:06.970531Z",
          "iopub.status.idle": "2024-10-29T01:10:06.979301Z",
          "shell.execute_reply.started": "2024-10-29T01:10:06.970495Z",
          "shell.execute_reply": "2024-10-29T01:10:06.978249Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to develop `TransformerClassifier` in which we input the embedding with the shape `[batch_size, seq_len, embed_dim]` to some `EncoderLayer` (i.e., num_layers specifies the number of EncoderLayer) and then compute the average of all token embeddings (i.e., `[batch_size, seq_len, embed_dim]`) across the `seq_len`. Finally, on the top of this average embedding, we build up a linear layer for making predictions."
      ],
      "metadata": {
        "id": "pI9I1Gl1ptq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, num_layers, dropout_rate=0.2, data_manager = None):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.vocab_size = data_manager.vocab_size\n",
        "        self.num_classes = data_manager.num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_seq_len = data_manager.max_seq_len\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "    def build(self):\n",
        "\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
        "        self.positional_encoding = PositionalEncoding(self.embed_dim, self.max_seq_len)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList()\n",
        "        for _ in range(self.num_layers):\n",
        "            self.encoder_layers.append(EncoderLayer(self.embed_dim, self.num_heads, self.ff_dim, self.dropout_rate))\n",
        "\n",
        "\n",
        "        self.fc = nn.Linear(self.embed_dim, self.num_classes)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "        x = torch.mean(x, dim=1)\n",
        "\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "VwzVfN2dpY_p",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:10:14.205067Z",
          "iopub.execute_input": "2024-10-29T01:10:14.205719Z",
          "iopub.status.idle": "2024-10-29T01:10:14.215277Z",
          "shell.execute_reply.started": "2024-10-29T01:10:14.205675Z",
          "shell.execute_reply": "2024-10-29T01:10:14.214277Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = TransformerClassifier(embed_dim=512, num_heads=8, ff_dim=2048, num_layers=12, dropout_rate=0.1, data_manager= dm)\n",
        "transformer.build()\n",
        "transformer = transformer.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
        "trainer = BaseTrainer(model= transformer, criterion=criterion, optimizer=optimizer, train_loader=dm.train_loader, val_loader=dm.valid_loader)\n",
        "trainer.fit(num_epochs=30)"
      ],
      "metadata": {
        "id": "V4sjIOzQrn3b",
        "outputId": "8375089f-d41e-4611-ed67-3cd131dcc637",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:10:22.248888Z",
          "iopub.execute_input": "2024-10-29T01:10:22.249277Z",
          "iopub.status.idle": "2024-10-29T01:12:42.777557Z",
          "shell.execute_reply.started": "2024-10-29T01:10:22.249239Z",
          "shell.execute_reply": "2024-10-29T01:12:42.776626Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/30\n26/26 - train_loss: 2.0195 - train_accuracy: 22.4235%                 - val_loss: 0.8066 - val_accuracy: 12.6263%\nEpoch 2/30\n26/26 - train_loss: 1.7232 - train_accuracy: 18.8007%                 - val_loss: 0.8273 - val_accuracy: 12.6263%\nEpoch 3/30\n26/26 - train_loss: 1.6846 - train_accuracy: 21.9238%                 - val_loss: 0.8405 - val_accuracy: 26.2626%\nEpoch 4/30\n26/26 - train_loss: 1.6877 - train_accuracy: 21.9863%                 - val_loss: 0.7704 - val_accuracy: 27.2727%\nEpoch 5/30\n26/26 - train_loss: 1.6988 - train_accuracy: 20.8620%                 - val_loss: 0.7289 - val_accuracy: 27.2727%\nEpoch 6/30\n26/26 - train_loss: 1.6803 - train_accuracy: 21.3616%                 - val_loss: 0.7095 - val_accuracy: 27.2727%\nEpoch 7/30\n26/26 - train_loss: 1.6396 - train_accuracy: 26.2961%                 - val_loss: 0.7326 - val_accuracy: 37.3737%\nEpoch 8/30\n26/26 - train_loss: 1.4001 - train_accuracy: 37.6640%                 - val_loss: 0.7161 - val_accuracy: 47.4747%\nEpoch 9/30\n26/26 - train_loss: 1.2422 - train_accuracy: 40.4747%                 - val_loss: 0.6288 - val_accuracy: 52.0202%\nEpoch 10/30\n26/26 - train_loss: 1.2179 - train_accuracy: 43.4104%                 - val_loss: 0.6564 - val_accuracy: 52.0202%\nEpoch 11/30\n26/26 - train_loss: 1.2150 - train_accuracy: 43.4728%                 - val_loss: 0.6537 - val_accuracy: 42.9293%\nEpoch 12/30\n26/26 - train_loss: 1.2459 - train_accuracy: 43.4104%                 - val_loss: 0.6519 - val_accuracy: 52.0202%\nEpoch 13/30\n26/26 - train_loss: 1.2114 - train_accuracy: 41.9738%                 - val_loss: 0.6387 - val_accuracy: 52.0202%\nEpoch 14/30\n26/26 - train_loss: 1.2317 - train_accuracy: 43.4104%                 - val_loss: 0.6480 - val_accuracy: 52.0202%\nEpoch 15/30\n26/26 - train_loss: 1.1640 - train_accuracy: 42.1611%                 - val_loss: 0.6213 - val_accuracy: 52.0202%\nEpoch 16/30\n26/26 - train_loss: 1.1949 - train_accuracy: 43.5978%                 - val_loss: 0.6206 - val_accuracy: 52.0202%\nEpoch 17/30\n26/26 - train_loss: 1.2376 - train_accuracy: 40.9119%                 - val_loss: 0.6298 - val_accuracy: 52.0202%\nEpoch 18/30\n26/26 - train_loss: 1.2266 - train_accuracy: 42.4110%                 - val_loss: 0.6463 - val_accuracy: 42.9293%\nEpoch 19/30\n26/26 - train_loss: 1.2405 - train_accuracy: 43.7227%                 - val_loss: 0.6809 - val_accuracy: 52.0202%\nEpoch 20/30\n26/26 - train_loss: 1.2180 - train_accuracy: 42.5984%                 - val_loss: 0.6839 - val_accuracy: 52.0202%\nEpoch 21/30\n26/26 - train_loss: 1.2252 - train_accuracy: 41.9738%                 - val_loss: 0.6617 - val_accuracy: 53.5354%\nEpoch 22/30\n26/26 - train_loss: 1.2002 - train_accuracy: 41.6615%                 - val_loss: 0.6299 - val_accuracy: 48.9899%\nEpoch 23/30\n26/26 - train_loss: 1.3626 - train_accuracy: 37.4766%                 - val_loss: 0.6581 - val_accuracy: 48.9899%\nEpoch 24/30\n26/26 - train_loss: 1.3150 - train_accuracy: 41.5990%                 - val_loss: 0.6316 - val_accuracy: 48.9899%\nEpoch 25/30\n26/26 - train_loss: 1.2868 - train_accuracy: 38.7883%                 - val_loss: 0.6526 - val_accuracy: 39.8990%\nEpoch 26/30\n26/26 - train_loss: 1.2311 - train_accuracy: 42.8482%                 - val_loss: 0.6746 - val_accuracy: 48.9899%\nEpoch 27/30\n26/26 - train_loss: 1.2266 - train_accuracy: 40.7870%                 - val_loss: 0.6670 - val_accuracy: 48.9899%\nEpoch 28/30\n26/26 - train_loss: 1.2843 - train_accuracy: 41.1618%                 - val_loss: 0.6952 - val_accuracy: 48.9899%\nEpoch 29/30\n26/26 - train_loss: 1.2764 - train_accuracy: 39.4129%                 - val_loss: 0.6139 - val_accuracy: 48.9899%\nEpoch 30/30\n26/26 - train_loss: 1.2872 - train_accuracy: 42.4735%                 - val_loss: 0.6796 - val_accuracy: 48.9899%\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"red\">**Question 4.2**</font>\n",
        "**Prefix prompt-tuning with Transformers: You need to implement the prefix prompt-tuning with Transformers. Basically, we base on a pre-trained Transformer, add prefix prompts, and do fine-tuning for a target dataset.**\n",
        "\n",
        "<div style=\"text-align: right\"><font color=\"red; font-weight:bold\">[Total marks for this part: 10 marks]<span></div>"
      ],
      "metadata": {
        "id": "aMbhi0_d0NL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement prefix prompt-tuning with pretrained Transformers, we first need to create the Bert dataset."
      ],
      "metadata": {
        "id": "UyecWD190TP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer, AdamW\n",
        "from datasets import Dataset\n",
        "\n",
        "model_name = \"bert-base-uncased\"  # BERT or any similar model\n",
        "\n",
        "# Tokenize input and prepare model inputs\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "dataset = Dataset.from_dict({\"text\": dm.str_questions, \"label\": dm.numeral_labels})\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length= 36)\n",
        "\n",
        "dataset = dataset.map(tokenize_function, batched=True)\n",
        "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "mq3PiV2UrueO",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:12:42.779161Z",
          "iopub.execute_input": "2024-10-29T01:12:42.779671Z",
          "iopub.status.idle": "2024-10-29T01:12:43.117025Z",
          "shell.execute_reply.started": "2024-10-29T01:12:42.779637Z",
          "shell.execute_reply": "2024-10-29T01:12:43.116216Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function splits the BERT dataset `dataset` into three BERT datasets for training, valid, and testing."
      ],
      "metadata": {
        "id": "ASKOrO5n1ckY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_valid_test_split(dataset, train_ratio=0.8, test_ratio = 0.1):\n",
        "    num_sentences = len(dataset)\n",
        "    train_size = int(num_sentences*train_ratio) +1\n",
        "    test_size = int(num_sentences*test_ratio) +1\n",
        "    valid_size = num_sentences - (train_size + test_size)\n",
        "    train_set = dataset[:train_size]\n",
        "    train_set = Dataset.from_dict(train_set)\n",
        "    train_set.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "    test_set = dataset[-test_size:]\n",
        "    test_set = Dataset.from_dict(test_set)\n",
        "    test_set.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "    valid_set = dataset[train_size:-test_size]\n",
        "    valid_set = Dataset.from_dict(valid_set)\n",
        "    valid_set.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
        "    valid_loader = DataLoader(valid_set, batch_size=64, shuffle=False)\n",
        "    return train_loader, test_loader, valid_loader\n"
      ],
      "metadata": {
        "id": "UDgfnCV70gzG",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:12:43.118631Z",
          "iopub.execute_input": "2024-10-29T01:12:43.118936Z",
          "iopub.status.idle": "2024-10-29T01:12:43.127167Z",
          "shell.execute_reply.started": "2024-10-29T01:12:43.118904Z",
          "shell.execute_reply": "2024-10-29T01:12:43.126316Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader, valid_loader = train_valid_test_split(dataset)"
      ],
      "metadata": {
        "id": "s_sEtXml1lT7",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:12:43.129030Z",
          "iopub.execute_input": "2024-10-29T01:12:43.129355Z",
          "iopub.status.idle": "2024-10-29T01:12:43.216197Z",
          "shell.execute_reply.started": "2024-10-29T01:12:43.129323Z",
          "shell.execute_reply": "2024-10-29T01:12:43.215477Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need to implement the class `PrefixTuningForClassification` for the prefix prompt fine-tuning. We first load a pre-trained BERT model specified by `model_name`. The parameter `prefix_length` specifies the length of the prefix prompts we add to the pre-trained BERT model. Specifically, given the input batch `[batch_size, seq_len]`, we input to the embedding layer of the pre-trained BERT model to obtain `[batch_size, seq_len, embed_size]`. We create the prefix prompts $P$ of the size `[prefix_length, embed_size]` and concatenate to the embeddings from the pre-trained BERT to obtain `[batch_size, seq_len + prefix_length, embed_size]`. This concatenation tensor will then be fed to the encoder layers of the pre-trained BERT layer to obtain the last `[batch_size, seq_len + prefix_length, embed_size]`."
      ],
      "metadata": {
        "id": "S3jHATGX2hn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then take mean across the seq_len to obtain `[batch_size, embed_size]` on which we can build up a linear layer for making predictions. Please note that **the parameters to tune include the prefix prompts $P$** and **the output linear layer**, and you should freeze the parameters of the BERT pre-trained model. Moreover, your code should cover the edge case when `prefix_length=None`. In this case, we do not insert any prefix prompts and we only do fine-tuning for the output linear layer on top.  "
      ],
      "metadata": {
        "id": "NXFogDf1jVD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PrefixTuningForClassification(nn.Module):\n",
        "    def __init__(self, model_name, prefix_length=None, data_manager = None):\n",
        "        super(PrefixTuningForClassification, self).__init__()\n",
        "\n",
        "        # Load the pretrained transformer model (BERT-like model)\n",
        "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
        "        self.hidden_size =  self.model.config.hidden_size\n",
        "        self.prefix_length = prefix_length\n",
        "        self.num_classes = data_manager.num_classes\n",
        "        self.build()\n",
        "\n",
        "    def build(self):\n",
        "        self.prefix_embeddings = torch.empty(self.prefix_length, self.hidden_size).to(device)\n",
        "        torch.nn.init.xavier_uniform_(self.prefix_embeddings)\n",
        "        self.prefix_embeddings.requires_grad = True\n",
        "\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.classifier = nn.Linear(self.hidden_size, self.num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        embeddings = self.model.embeddings(input_ids)\n",
        "        prefix_embeddings_expanded = self.prefix_embeddings.unsqueeze(0).expand(embeddings.shape[0], -1, -1)\n",
        "        embeddings = torch.cat((prefix_embeddings_expanded, embeddings), dim=1)\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        extended_attention_mask = torch.cat(\n",
        "            (torch.ones(batch_size, self.prefix_length, device=attention_mask.device), attention_mask),\n",
        "            dim=1\n",
        "        )\n",
        "        extended_attention_mask = self.model.get_extended_attention_mask(extended_attention_mask, (batch_size, seq_len + self.prefix_length), attention_mask.device)\n",
        "        x = self.model.encoder(embeddings, attention_mask=extended_attention_mask).last_hidden_state\n",
        "        x = torch.mean(x, dim=1)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "bZtGBqQV16Kj",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:27:08.382995Z",
          "iopub.execute_input": "2024-10-29T01:27:08.383831Z",
          "iopub.status.idle": "2024-10-29T01:27:08.394703Z",
          "shell.execute_reply.started": "2024-10-29T01:27:08.383768Z",
          "shell.execute_reply": "2024-10-29T01:27:08.393735Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m7w7qOdq602o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the following `FineTunedBaseTrainer` to train the prompt fine-tuning models."
      ],
      "metadata": {
        "id": "RtYVNYXi24Os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FineTunedBaseTrainer:\n",
        "    def __init__(self, model, criterion, optimizer, train_loader, val_loader):\n",
        "        self.model = model\n",
        "        self.criterion = criterion  #the loss function\n",
        "        self.optimizer = optimizer  #the optimizer\n",
        "        self.train_loader = train_loader  #the train loader\n",
        "        self.val_loader = val_loader  #the valid loader\n",
        "\n",
        "    #the function to train the model in many epochs\n",
        "    def fit(self, num_epochs):\n",
        "        self.num_batches = len(self.train_loader)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "            train_loss, train_accuracy = self.train_one_epoch()\n",
        "            val_loss, val_accuracy = self.validate_one_epoch()\n",
        "            print(\n",
        "                f'{self.num_batches}/{self.num_batches} - train_loss: {train_loss:.4f} - train_accuracy: {train_accuracy*100:.4f}% \\\n",
        "                - val_loss: {val_loss:.4f} - val_accuracy: {val_accuracy*100:.4f}%')\n",
        "\n",
        "    #train in one epoch, return the train_acc, train_loss\n",
        "    def train_one_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "        for batch in self.train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(input_ids= input_ids, attention_mask= attention_mask)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        train_accuracy = correct / total\n",
        "        train_loss = running_loss / self.num_batches\n",
        "        return train_loss, train_accuracy\n",
        "\n",
        "    #evaluate on a loader and return the loss and accuracy\n",
        "    def evaluate(self, loader):\n",
        "        self.model.eval()\n",
        "        loss, correct, total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for batch in loader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                labels = batch[\"label\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                outputs = self.model(input_ids= input_ids, attention_mask= attention_mask)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = correct / total\n",
        "        loss = loss / len(self.val_loader)\n",
        "        return loss, accuracy\n",
        "\n",
        "    #return the val_acc, val_loss, be called at the end of each epoch\n",
        "    def validate_one_epoch(self):\n",
        "      val_loss, val_accuracy = self.evaluate(self.val_loader)\n",
        "      return val_loss, val_accuracy"
      ],
      "metadata": {
        "id": "NmRkMqYB24tv",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:12:43.217386Z",
          "iopub.execute_input": "2024-10-29T01:12:43.217739Z",
          "iopub.status.idle": "2024-10-29T01:12:43.235109Z",
          "shell.execute_reply.started": "2024-10-29T01:12:43.217699Z",
          "shell.execute_reply": "2024-10-29T01:12:43.234260Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We declare and train the prefix-prompt tuning model. In addition, you need to be patient with this model because it might converge slowly with many epochs."
      ],
      "metadata": {
        "id": "4KQhBHPe20zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix_tuning_model = PrefixTuningForClassification(model_name = \"bert-base-uncased\", prefix_length = 5, data_manager = dm).to(device)"
      ],
      "metadata": {
        "id": "qLowEOeg3HWW",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:27:14.101388Z",
          "iopub.execute_input": "2024-10-29T01:27:14.102482Z",
          "iopub.status.idle": "2024-10-29T01:27:16.750463Z",
          "shell.execute_reply.started": "2024-10-29T01:27:14.102431Z",
          "shell.execute_reply": "2024-10-29T01:27:16.749552Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if prefix_tuning_model.prefix_length is not None:\n",
        "  optimizer = torch.optim.Adam(list(prefix_tuning_model.classifier.parameters()) + [prefix_tuning_model.prefix_embeddings], lr=5e-5)\n",
        "else:\n",
        "  optimizer = torch.optim.Adam(prefix_tuning_model.classifier.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "trainer = FineTunedBaseTrainer(model= prefix_tuning_model, criterion=criterion, optimizer=optimizer, train_loader=train_loader, val_loader=valid_loader)\n",
        "trainer.fit(num_epochs=100)"
      ],
      "metadata": {
        "id": "UYMDOjWW3UVk",
        "outputId": "648b9670-0b75-4339-94ed-69483d7cafc8",
        "execution": {
          "iopub.status.busy": "2024-10-28T22:37:58.349297Z",
          "iopub.execute_input": "2024-10-28T22:37:58.349609Z",
          "iopub.status.idle": "2024-10-28T22:52:07.595623Z",
          "shell.execute_reply.started": "2024-10-28T22:37:58.349577Z",
          "shell.execute_reply": "2024-10-28T22:52:07.594655Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1141: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "26/26 - train_loss: 1.7959 - train_accuracy: 20.6121%                 - val_loss: 0.9391 - val_accuracy: 19.6970%\nEpoch 2/100\n26/26 - train_loss: 1.7557 - train_accuracy: 25.2967%                 - val_loss: 0.9187 - val_accuracy: 26.2626%\nEpoch 3/100\n26/26 - train_loss: 1.7322 - train_accuracy: 28.0450%                 - val_loss: 0.9033 - val_accuracy: 29.7980%\nEpoch 4/100\n26/26 - train_loss: 1.7085 - train_accuracy: 31.5428%                 - val_loss: 0.8883 - val_accuracy: 32.3232%\nEpoch 5/100\n26/26 - train_loss: 1.6836 - train_accuracy: 35.6027%                 - val_loss: 0.8740 - val_accuracy: 34.8485%\nEpoch 6/100\n26/26 - train_loss: 1.6524 - train_accuracy: 36.7270%                 - val_loss: 0.8642 - val_accuracy: 36.3636%\nEpoch 7/100\n26/26 - train_loss: 1.6373 - train_accuracy: 38.9756%                 - val_loss: 0.8553 - val_accuracy: 37.3737%\nEpoch 8/100\n26/26 - train_loss: 1.6101 - train_accuracy: 39.7252%                 - val_loss: 0.8450 - val_accuracy: 39.8990%\nEpoch 9/100\n26/26 - train_loss: 1.6067 - train_accuracy: 40.0999%                 - val_loss: 0.8360 - val_accuracy: 38.8889%\nEpoch 10/100\n26/26 - train_loss: 1.5970 - train_accuracy: 43.3479%                 - val_loss: 0.8217 - val_accuracy: 45.9596%\nEpoch 11/100\n26/26 - train_loss: 1.5728 - train_accuracy: 45.0968%                 - val_loss: 0.8144 - val_accuracy: 47.9798%\nEpoch 12/100\n26/26 - train_loss: 1.5603 - train_accuracy: 44.5347%                 - val_loss: 0.8072 - val_accuracy: 48.4848%\nEpoch 13/100\n26/26 - train_loss: 1.5487 - train_accuracy: 47.4079%                 - val_loss: 0.7943 - val_accuracy: 51.5152%\nEpoch 14/100\n26/26 - train_loss: 1.5241 - train_accuracy: 48.5946%                 - val_loss: 0.7892 - val_accuracy: 50.5051%\nEpoch 15/100\n26/26 - train_loss: 1.5149 - train_accuracy: 49.9063%                 - val_loss: 0.7806 - val_accuracy: 51.0101%\nEpoch 16/100\n26/26 - train_loss: 1.5219 - train_accuracy: 49.2817%                 - val_loss: 0.7718 - val_accuracy: 47.9798%\nEpoch 17/100\n26/26 - train_loss: 1.4964 - train_accuracy: 50.1562%                 - val_loss: 0.7672 - val_accuracy: 50.5051%\nEpoch 18/100\n26/26 - train_loss: 1.4953 - train_accuracy: 50.7808%                 - val_loss: 0.7671 - val_accuracy: 50.5051%\nEpoch 19/100\n26/26 - train_loss: 1.4801 - train_accuracy: 50.8432%                 - val_loss: 0.7612 - val_accuracy: 52.0202%\nEpoch 20/100\n26/26 - train_loss: 1.4795 - train_accuracy: 52.0924%                 - val_loss: 0.7547 - val_accuracy: 53.0303%\nEpoch 21/100\n26/26 - train_loss: 1.4440 - train_accuracy: 54.4035%                 - val_loss: 0.7483 - val_accuracy: 54.5455%\nEpoch 22/100\n26/26 - train_loss: 1.4463 - train_accuracy: 56.0275%                 - val_loss: 0.7452 - val_accuracy: 54.0404%\nEpoch 23/100\n26/26 - train_loss: 1.4327 - train_accuracy: 55.5903%                 - val_loss: 0.7438 - val_accuracy: 54.5455%\nEpoch 24/100\n26/26 - train_loss: 1.4299 - train_accuracy: 55.5903%                 - val_loss: 0.7389 - val_accuracy: 54.5455%\nEpoch 25/100\n26/26 - train_loss: 1.4034 - train_accuracy: 56.0275%                 - val_loss: 0.7307 - val_accuracy: 56.0606%\nEpoch 26/100\n26/26 - train_loss: 1.3939 - train_accuracy: 58.4010%                 - val_loss: 0.7222 - val_accuracy: 55.5556%\nEpoch 27/100\n26/26 - train_loss: 1.3959 - train_accuracy: 58.2136%                 - val_loss: 0.7185 - val_accuracy: 57.0707%\nEpoch 28/100\n26/26 - train_loss: 1.3660 - train_accuracy: 58.4010%                 - val_loss: 0.7121 - val_accuracy: 57.5758%\nEpoch 29/100\n26/26 - train_loss: 1.3733 - train_accuracy: 58.7758%                 - val_loss: 0.7048 - val_accuracy: 59.0909%\nEpoch 30/100\n26/26 - train_loss: 1.3643 - train_accuracy: 61.0244%                 - val_loss: 0.6990 - val_accuracy: 59.0909%\nEpoch 31/100\n26/26 - train_loss: 1.3413 - train_accuracy: 61.3367%                 - val_loss: 0.6950 - val_accuracy: 59.0909%\nEpoch 32/100\n26/26 - train_loss: 1.3309 - train_accuracy: 61.0868%                 - val_loss: 0.6950 - val_accuracy: 59.0909%\nEpoch 33/100\n26/26 - train_loss: 1.3378 - train_accuracy: 59.7751%                 - val_loss: 0.6939 - val_accuracy: 61.1111%\nEpoch 34/100\n26/26 - train_loss: 1.3254 - train_accuracy: 63.3354%                 - val_loss: 0.6862 - val_accuracy: 63.6364%\nEpoch 35/100\n26/26 - train_loss: 1.3110 - train_accuracy: 62.9606%                 - val_loss: 0.6782 - val_accuracy: 62.6263%\nEpoch 36/100\n26/26 - train_loss: 1.3045 - train_accuracy: 62.6483%                 - val_loss: 0.6693 - val_accuracy: 63.1313%\nEpoch 37/100\n26/26 - train_loss: 1.2898 - train_accuracy: 63.0856%                 - val_loss: 0.6646 - val_accuracy: 62.6263%\nEpoch 38/100\n26/26 - train_loss: 1.2926 - train_accuracy: 63.3354%                 - val_loss: 0.6614 - val_accuracy: 63.1313%\nEpoch 39/100\n26/26 - train_loss: 1.2781 - train_accuracy: 65.0219%                 - val_loss: 0.6553 - val_accuracy: 63.1313%\nEpoch 40/100\n26/26 - train_loss: 1.2623 - train_accuracy: 64.4597%                 - val_loss: 0.6471 - val_accuracy: 65.1515%\nEpoch 41/100\n26/26 - train_loss: 1.2403 - train_accuracy: 65.8339%                 - val_loss: 0.6417 - val_accuracy: 64.6465%\nEpoch 42/100\n26/26 - train_loss: 1.2538 - train_accuracy: 64.3973%                 - val_loss: 0.6407 - val_accuracy: 64.6465%\nEpoch 43/100\n26/26 - train_loss: 1.2477 - train_accuracy: 66.5834%                 - val_loss: 0.6371 - val_accuracy: 64.6465%\nEpoch 44/100\n26/26 - train_loss: 1.2471 - train_accuracy: 66.9582%                 - val_loss: 0.6345 - val_accuracy: 65.1515%\nEpoch 45/100\n26/26 - train_loss: 1.2173 - train_accuracy: 67.8326%                 - val_loss: 0.6276 - val_accuracy: 65.6566%\nEpoch 46/100\n26/26 - train_loss: 1.2097 - train_accuracy: 69.1443%                 - val_loss: 0.6243 - val_accuracy: 67.1717%\nEpoch 47/100\n26/26 - train_loss: 1.2001 - train_accuracy: 68.7071%                 - val_loss: 0.6167 - val_accuracy: 69.1919%\nEpoch 48/100\n26/26 - train_loss: 1.1818 - train_accuracy: 70.3310%                 - val_loss: 0.6072 - val_accuracy: 69.1919%\nEpoch 49/100\n26/26 - train_loss: 1.1783 - train_accuracy: 69.1443%                 - val_loss: 0.6017 - val_accuracy: 69.1919%\nEpoch 50/100\n26/26 - train_loss: 1.1826 - train_accuracy: 69.5191%                 - val_loss: 0.5974 - val_accuracy: 69.1919%\nEpoch 51/100\n26/26 - train_loss: 1.1776 - train_accuracy: 68.3948%                 - val_loss: 0.5918 - val_accuracy: 69.6970%\nEpoch 52/100\n26/26 - train_loss: 1.1602 - train_accuracy: 70.6433%                 - val_loss: 0.5856 - val_accuracy: 70.7071%\nEpoch 53/100\n26/26 - train_loss: 1.1333 - train_accuracy: 71.0806%                 - val_loss: 0.5772 - val_accuracy: 71.2121%\nEpoch 54/100\n26/26 - train_loss: 1.1442 - train_accuracy: 70.1437%                 - val_loss: 0.5763 - val_accuracy: 71.2121%\nEpoch 55/100\n26/26 - train_loss: 1.1287 - train_accuracy: 71.3929%                 - val_loss: 0.5706 - val_accuracy: 71.2121%\nEpoch 56/100\n26/26 - train_loss: 1.1092 - train_accuracy: 73.1418%                 - val_loss: 0.5649 - val_accuracy: 72.7273%\nEpoch 57/100\n26/26 - train_loss: 1.1119 - train_accuracy: 73.1418%                 - val_loss: 0.5576 - val_accuracy: 71.7172%\nEpoch 58/100\n26/26 - train_loss: 1.0975 - train_accuracy: 71.2680%                 - val_loss: 0.5553 - val_accuracy: 72.2222%\nEpoch 59/100\n26/26 - train_loss: 1.1227 - train_accuracy: 71.8301%                 - val_loss: 0.5481 - val_accuracy: 72.7273%\nEpoch 60/100\n26/26 - train_loss: 1.1017 - train_accuracy: 73.6415%                 - val_loss: 0.5390 - val_accuracy: 74.2424%\nEpoch 61/100\n26/26 - train_loss: 1.0791 - train_accuracy: 73.5166%                 - val_loss: 0.5321 - val_accuracy: 74.2424%\nEpoch 62/100\n26/26 - train_loss: 1.0759 - train_accuracy: 73.4541%                 - val_loss: 0.5264 - val_accuracy: 74.2424%\nEpoch 63/100\n26/26 - train_loss: 1.0526 - train_accuracy: 73.3292%                 - val_loss: 0.5199 - val_accuracy: 74.2424%\nEpoch 64/100\n26/26 - train_loss: 1.0568 - train_accuracy: 73.8289%                 - val_loss: 0.5124 - val_accuracy: 73.7374%\nEpoch 65/100\n26/26 - train_loss: 1.0818 - train_accuracy: 75.7027%                 - val_loss: 0.5070 - val_accuracy: 74.2424%\nEpoch 66/100\n26/26 - train_loss: 1.0455 - train_accuracy: 74.0787%                 - val_loss: 0.5027 - val_accuracy: 74.2424%\nEpoch 67/100\n26/26 - train_loss: 1.0433 - train_accuracy: 76.5771%                 - val_loss: 0.5014 - val_accuracy: 74.7475%\nEpoch 68/100\n26/26 - train_loss: 1.0194 - train_accuracy: 75.0156%                 - val_loss: 0.4947 - val_accuracy: 74.7475%\nEpoch 69/100\n26/26 - train_loss: 1.0387 - train_accuracy: 75.8901%                 - val_loss: 0.4875 - val_accuracy: 75.2525%\nEpoch 70/100\n26/26 - train_loss: 1.0064 - train_accuracy: 75.5778%                 - val_loss: 0.4802 - val_accuracy: 75.7576%\nEpoch 71/100\n26/26 - train_loss: 0.9955 - train_accuracy: 75.2030%                 - val_loss: 0.4746 - val_accuracy: 75.2525%\nEpoch 72/100\n26/26 - train_loss: 0.9914 - train_accuracy: 75.8276%                 - val_loss: 0.4717 - val_accuracy: 74.7475%\nEpoch 73/100\n26/26 - train_loss: 0.9926 - train_accuracy: 76.5771%                 - val_loss: 0.4635 - val_accuracy: 75.2525%\nEpoch 74/100\n26/26 - train_loss: 0.9713 - train_accuracy: 76.0150%                 - val_loss: 0.4577 - val_accuracy: 76.2626%\nEpoch 75/100\n26/26 - train_loss: 0.9926 - train_accuracy: 77.8888%                 - val_loss: 0.4510 - val_accuracy: 76.7677%\nEpoch 76/100\n26/26 - train_loss: 0.9455 - train_accuracy: 76.4522%                 - val_loss: 0.4441 - val_accuracy: 77.2727%\nEpoch 77/100\n26/26 - train_loss: 0.9593 - train_accuracy: 76.8270%                 - val_loss: 0.4413 - val_accuracy: 77.2727%\nEpoch 78/100\n26/26 - train_loss: 0.9419 - train_accuracy: 77.0768%                 - val_loss: 0.4401 - val_accuracy: 76.2626%\nEpoch 79/100\n26/26 - train_loss: 0.9685 - train_accuracy: 75.8901%                 - val_loss: 0.4349 - val_accuracy: 76.7677%\nEpoch 80/100\n26/26 - train_loss: 0.9578 - train_accuracy: 77.2642%                 - val_loss: 0.4340 - val_accuracy: 77.7778%\nEpoch 81/100\n26/26 - train_loss: 0.9223 - train_accuracy: 76.3273%                 - val_loss: 0.4253 - val_accuracy: 76.7677%\nEpoch 82/100\n26/26 - train_loss: 0.9235 - train_accuracy: 78.4510%                 - val_loss: 0.4196 - val_accuracy: 76.2626%\nEpoch 83/100\n26/26 - train_loss: 0.9166 - train_accuracy: 77.2017%                 - val_loss: 0.4164 - val_accuracy: 77.2727%\nEpoch 84/100\n26/26 - train_loss: 0.9110 - train_accuracy: 76.8894%                 - val_loss: 0.4128 - val_accuracy: 78.2828%\nEpoch 85/100\n26/26 - train_loss: 0.9401 - train_accuracy: 77.8264%                 - val_loss: 0.4079 - val_accuracy: 80.3030%\nEpoch 86/100\n26/26 - train_loss: 0.8963 - train_accuracy: 78.3885%                 - val_loss: 0.4014 - val_accuracy: 80.3030%\nEpoch 87/100\n26/26 - train_loss: 0.8714 - train_accuracy: 78.2636%                 - val_loss: 0.3954 - val_accuracy: 80.3030%\nEpoch 88/100\n26/26 - train_loss: 0.8797 - train_accuracy: 78.1387%                 - val_loss: 0.3910 - val_accuracy: 80.3030%\nEpoch 89/100\n26/26 - train_loss: 0.9245 - train_accuracy: 79.2630%                 - val_loss: 0.3868 - val_accuracy: 79.7980%\nEpoch 90/100\n26/26 - train_loss: 0.8751 - train_accuracy: 78.6384%                 - val_loss: 0.3813 - val_accuracy: 79.7980%\nEpoch 91/100\n26/26 - train_loss: 0.9118 - train_accuracy: 79.4503%                 - val_loss: 0.3788 - val_accuracy: 80.3030%\nEpoch 92/100\n26/26 - train_loss: 0.8812 - train_accuracy: 79.3254%                 - val_loss: 0.3752 - val_accuracy: 80.8081%\nEpoch 93/100\n26/26 - train_loss: 0.8494 - train_accuracy: 79.7002%                 - val_loss: 0.3731 - val_accuracy: 80.8081%\nEpoch 94/100\n26/26 - train_loss: 0.8622 - train_accuracy: 79.0756%                 - val_loss: 0.3694 - val_accuracy: 80.8081%\nEpoch 95/100\n26/26 - train_loss: 0.8334 - train_accuracy: 79.0131%                 - val_loss: 0.3646 - val_accuracy: 81.8182%\nEpoch 96/100\n26/26 - train_loss: 0.8652 - train_accuracy: 80.0750%                 - val_loss: 0.3589 - val_accuracy: 81.3131%\nEpoch 97/100\n26/26 - train_loss: 0.8536 - train_accuracy: 80.2623%                 - val_loss: 0.3581 - val_accuracy: 81.8182%\nEpoch 98/100\n26/26 - train_loss: 0.8399 - train_accuracy: 79.7626%                 - val_loss: 0.3574 - val_accuracy: 81.3131%\nEpoch 99/100\n26/26 - train_loss: 0.8315 - train_accuracy: 79.7002%                 - val_loss: 0.3508 - val_accuracy: 81.8182%\nEpoch 100/100\n26/26 - train_loss: 0.8195 - train_accuracy: 80.8245%                 - val_loss: 0.3485 - val_accuracy: 82.3232%\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"red\">**Question 4.3**</font>\n",
        "**For any models defined in the previous questions (of all parts), you are free to fine-tune hyperparameters, e.g., `optimizer`, `learning_rate`, `state_sizes`, such that you get a best model, i.e., the one with the highest accuracy on the test set. You will need to report (i) what is your best model,  (ii) its accuracy on the test set, and (iii) the values of its hyperparameters. Note that you must report your best model's accuracy with rounding to 4 decimal places, i.e., 0.xxxx. You will also need to upload your best model (or provide us with the link to download your best model). The assessment will be based on your best model's accuracy, with up to 9 marks available, specifically:**\n",
        "* The best accuracy $\\ge$ 0.97: 10 marks\n",
        "* 0.97 $>$ The best accuracy $\\ge$ 0.92: 7 marks\n",
        "* 0.92 $>$ The best accuracy $\\ge$ 0.85: 4 marks\n",
        "* The best accuracy $<$ 0.85: 0 mark\n",
        "\n",
        "**For this question, you can put below the code to train the best model. In this case, you need to show your code and the evidence of running regarding the best model. Moreover, if you save the best model, you need to provide the link to download the best model, the code to load the best model, and then evaluate on the test set.**\n",
        "<div style=\"text-align: right\"><font color=\"red\">[10 marks]</font></div>"
      ],
      "metadata": {
        "id": "xi6PnvdO-Glw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Give your answer here.\n",
        "\n",
        "(i) What is your best model?\n",
        "\n",
        "PrefixTuningForClassification from the base model bert-base-uncased\n",
        "\n",
        "(ii) The accuracy of your best model on the test set\n",
        "\n",
        "97.4747%\n",
        "\n",
        "(iii) The values of the hyperparameters of your best model\n",
        "\n",
        "prefix_length = 20, learning rate=4e-5, 543 epochs\n",
        "\n",
        "(iv) The link to download your best model\n",
        "\n",
        "https://github.com/TheAlchemist010/DeepLearning-Notebooks/raw/refs/heads/main/FIT3181/model.pth\n",
        "\n",
        "Code to download and test below"
      ],
      "metadata": {
        "id": "EvM2pq3J-K0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initial training\n",
        "prefix_tuning_model = PrefixTuningForClassification(model_name = \"bert-base-uncased\", prefix_length = 20, data_manager = dm).to(device)\n",
        "optimizer = torch.optim.Adam(list(prefix_tuning_model.classifier.parameters()) + [prefix_tuning_model.prefix_embeddings], lr=4e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "trainer = FineTunedBaseTrainer(model= prefix_tuning_model, criterion=criterion, optimizer=optimizer, train_loader=train_loader, val_loader=valid_loader)\n",
        "trainer.fit(num_epochs=400)\n",
        "\n",
        "#trainer.fit(num_epochs=400) is later ran again to reach the accuraccy"
      ],
      "metadata": {
        "id": "l3NqVk5aF5vb",
        "outputId": "4a9e7a1d-ccef-488f-e05b-92a7b093f858",
        "execution": {
          "iopub.status.busy": "2024-10-29T01:27:25.359840Z",
          "iopub.execute_input": "2024-10-29T01:27:25.360702Z",
          "iopub.status.idle": "2024-10-29T02:37:12.368836Z",
          "shell.execute_reply.started": "2024-10-29T01:27:25.360658Z",
          "shell.execute_reply": "2024-10-29T02:37:12.367849Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/400\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1141: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "26/26 - train_loss: 1.7645 - train_accuracy: 20.4872%                 - val_loss: 0.9119 - val_accuracy: 18.6869%\nEpoch 2/400\n26/26 - train_loss: 1.7526 - train_accuracy: 20.7370%                 - val_loss: 0.8934 - val_accuracy: 18.6869%\nEpoch 3/400\n26/26 - train_loss: 1.7244 - train_accuracy: 21.7364%                 - val_loss: 0.8753 - val_accuracy: 19.6970%\nEpoch 4/400\n26/26 - train_loss: 1.7243 - train_accuracy: 24.3598%                 - val_loss: 0.8630 - val_accuracy: 23.2323%\nEpoch 5/400\n26/26 - train_loss: 1.6869 - train_accuracy: 25.6715%                 - val_loss: 0.8545 - val_accuracy: 26.2626%\nEpoch 6/400\n26/26 - train_loss: 1.6720 - train_accuracy: 29.4191%                 - val_loss: 0.8469 - val_accuracy: 30.8081%\nEpoch 7/400\n26/26 - train_loss: 1.6759 - train_accuracy: 31.6677%                 - val_loss: 0.8416 - val_accuracy: 30.8081%\nEpoch 8/400\n26/26 - train_loss: 1.6481 - train_accuracy: 33.9163%                 - val_loss: 0.8342 - val_accuracy: 32.8283%\nEpoch 9/400\n26/26 - train_loss: 1.6395 - train_accuracy: 35.1655%                 - val_loss: 0.8300 - val_accuracy: 34.8485%\nEpoch 10/400\n26/26 - train_loss: 1.6276 - train_accuracy: 37.2892%                 - val_loss: 0.8197 - val_accuracy: 40.4040%\nEpoch 11/400\n26/26 - train_loss: 1.6231 - train_accuracy: 39.4753%                 - val_loss: 0.8098 - val_accuracy: 40.9091%\nEpoch 12/400\n26/26 - train_loss: 1.6136 - train_accuracy: 40.6621%                 - val_loss: 0.8091 - val_accuracy: 41.4141%\nEpoch 13/400\n26/26 - train_loss: 1.6132 - train_accuracy: 38.7883%                 - val_loss: 0.8092 - val_accuracy: 41.9192%\nEpoch 14/400\n26/26 - train_loss: 1.5987 - train_accuracy: 40.7245%                 - val_loss: 0.8103 - val_accuracy: 43.9394%\nEpoch 15/400\n26/26 - train_loss: 1.5881 - train_accuracy: 40.3498%                 - val_loss: 0.8088 - val_accuracy: 42.9293%\nEpoch 16/400\n26/26 - train_loss: 1.5858 - train_accuracy: 40.8495%                 - val_loss: 0.8022 - val_accuracy: 43.9394%\nEpoch 17/400\n26/26 - train_loss: 1.5921 - train_accuracy: 41.5365%                 - val_loss: 0.8028 - val_accuracy: 44.9495%\nEpoch 18/400\n26/26 - train_loss: 1.5627 - train_accuracy: 43.8476%                 - val_loss: 0.8025 - val_accuracy: 44.9495%\nEpoch 19/400\n26/26 - train_loss: 1.5747 - train_accuracy: 42.7858%                 - val_loss: 0.8059 - val_accuracy: 44.9495%\nEpoch 20/400\n26/26 - train_loss: 1.5522 - train_accuracy: 43.9101%                 - val_loss: 0.8041 - val_accuracy: 44.4444%\nEpoch 21/400\n26/26 - train_loss: 1.5482 - train_accuracy: 42.6608%                 - val_loss: 0.8060 - val_accuracy: 43.9394%\nEpoch 22/400\n26/26 - train_loss: 1.5379 - train_accuracy: 44.4097%                 - val_loss: 0.8081 - val_accuracy: 42.4242%\nEpoch 23/400\n26/26 - train_loss: 1.5274 - train_accuracy: 44.5347%                 - val_loss: 0.8076 - val_accuracy: 42.4242%\nEpoch 24/400\n26/26 - train_loss: 1.5424 - train_accuracy: 44.8470%                 - val_loss: 0.8082 - val_accuracy: 42.9293%\nEpoch 25/400\n26/26 - train_loss: 1.5251 - train_accuracy: 43.2230%                 - val_loss: 0.8063 - val_accuracy: 43.9394%\nEpoch 26/400\n26/26 - train_loss: 1.5230 - train_accuracy: 44.5971%                 - val_loss: 0.8058 - val_accuracy: 43.9394%\nEpoch 27/400\n26/26 - train_loss: 1.5118 - train_accuracy: 46.2211%                 - val_loss: 0.8044 - val_accuracy: 45.4545%\nEpoch 28/400\n26/26 - train_loss: 1.4935 - train_accuracy: 44.9719%                 - val_loss: 0.8041 - val_accuracy: 44.4444%\nEpoch 29/400\n26/26 - train_loss: 1.4995 - train_accuracy: 45.9088%                 - val_loss: 0.8063 - val_accuracy: 44.4444%\nEpoch 30/400\n26/26 - train_loss: 1.4777 - train_accuracy: 46.5959%                 - val_loss: 0.8058 - val_accuracy: 45.4545%\nEpoch 31/400\n26/26 - train_loss: 1.4707 - train_accuracy: 47.0956%                 - val_loss: 0.8057 - val_accuracy: 45.9596%\nEpoch 32/400\n26/26 - train_loss: 1.4791 - train_accuracy: 47.0956%                 - val_loss: 0.8063 - val_accuracy: 45.9596%\nEpoch 33/400\n26/26 - train_loss: 1.4431 - train_accuracy: 47.5328%                 - val_loss: 0.8055 - val_accuracy: 46.9697%\nEpoch 34/400\n26/26 - train_loss: 1.4586 - train_accuracy: 47.9076%                 - val_loss: 0.7986 - val_accuracy: 46.9697%\nEpoch 35/400\n26/26 - train_loss: 1.4608 - train_accuracy: 46.7208%                 - val_loss: 0.7980 - val_accuracy: 47.9798%\nEpoch 36/400\n26/26 - train_loss: 1.4137 - train_accuracy: 48.7820%                 - val_loss: 0.8021 - val_accuracy: 47.4747%\nEpoch 37/400\n26/26 - train_loss: 1.4487 - train_accuracy: 47.7202%                 - val_loss: 0.7925 - val_accuracy: 47.9798%\nEpoch 38/400\n26/26 - train_loss: 1.4062 - train_accuracy: 48.5946%                 - val_loss: 0.7859 - val_accuracy: 47.9798%\nEpoch 39/400\n26/26 - train_loss: 1.4069 - train_accuracy: 48.4072%                 - val_loss: 0.7840 - val_accuracy: 48.4848%\nEpoch 40/400\n26/26 - train_loss: 1.4179 - train_accuracy: 48.3448%                 - val_loss: 0.7800 - val_accuracy: 47.9798%\nEpoch 41/400\n26/26 - train_loss: 1.3928 - train_accuracy: 49.9063%                 - val_loss: 0.7806 - val_accuracy: 46.9697%\nEpoch 42/400\n26/26 - train_loss: 1.4028 - train_accuracy: 49.5315%                 - val_loss: 0.7742 - val_accuracy: 47.4747%\nEpoch 43/400\n26/26 - train_loss: 1.3622 - train_accuracy: 51.4054%                 - val_loss: 0.7751 - val_accuracy: 47.9798%\nEpoch 44/400\n26/26 - train_loss: 1.3870 - train_accuracy: 49.9063%                 - val_loss: 0.7641 - val_accuracy: 46.9697%\nEpoch 45/400\n26/26 - train_loss: 1.3380 - train_accuracy: 49.7814%                 - val_loss: 0.7665 - val_accuracy: 47.9798%\nEpoch 46/400\n26/26 - train_loss: 1.3786 - train_accuracy: 49.2817%                 - val_loss: 0.7669 - val_accuracy: 47.4747%\nEpoch 47/400\n26/26 - train_loss: 1.3491 - train_accuracy: 51.9051%                 - val_loss: 0.7628 - val_accuracy: 47.9798%\nEpoch 48/400\n26/26 - train_loss: 1.3699 - train_accuracy: 51.1555%                 - val_loss: 0.7598 - val_accuracy: 47.4747%\nEpoch 49/400\n26/26 - train_loss: 1.3534 - train_accuracy: 52.0924%                 - val_loss: 0.7610 - val_accuracy: 50.0000%\nEpoch 50/400\n26/26 - train_loss: 1.3504 - train_accuracy: 52.7795%                 - val_loss: 0.7587 - val_accuracy: 50.5051%\nEpoch 51/400\n26/26 - train_loss: 1.3184 - train_accuracy: 53.4666%                 - val_loss: 0.7508 - val_accuracy: 50.5051%\nEpoch 52/400\n26/26 - train_loss: 1.3255 - train_accuracy: 54.1537%                 - val_loss: 0.7462 - val_accuracy: 50.5051%\nEpoch 53/400\n26/26 - train_loss: 1.3172 - train_accuracy: 53.5290%                 - val_loss: 0.7447 - val_accuracy: 50.5051%\nEpoch 54/400\n26/26 - train_loss: 1.3151 - train_accuracy: 54.9032%                 - val_loss: 0.7384 - val_accuracy: 50.5051%\nEpoch 55/400\n26/26 - train_loss: 1.2897 - train_accuracy: 55.6527%                 - val_loss: 0.7179 - val_accuracy: 50.5051%\nEpoch 56/400\n26/26 - train_loss: 1.2787 - train_accuracy: 54.6533%                 - val_loss: 0.7187 - val_accuracy: 49.4949%\nEpoch 57/400\n26/26 - train_loss: 1.2742 - train_accuracy: 55.9650%                 - val_loss: 0.7265 - val_accuracy: 50.5051%\nEpoch 58/400\n26/26 - train_loss: 1.2880 - train_accuracy: 56.1524%                 - val_loss: 0.7060 - val_accuracy: 52.5253%\nEpoch 59/400\n26/26 - train_loss: 1.2655 - train_accuracy: 57.3392%                 - val_loss: 0.7023 - val_accuracy: 53.0303%\nEpoch 60/400\n26/26 - train_loss: 1.2700 - train_accuracy: 56.9019%                 - val_loss: 0.7007 - val_accuracy: 53.0303%\nEpoch 61/400\n26/26 - train_loss: 1.2448 - train_accuracy: 58.7758%                 - val_loss: 0.7004 - val_accuracy: 53.0303%\nEpoch 62/400\n26/26 - train_loss: 1.2354 - train_accuracy: 57.5265%                 - val_loss: 0.6978 - val_accuracy: 53.0303%\nEpoch 63/400\n26/26 - train_loss: 1.2548 - train_accuracy: 57.7139%                 - val_loss: 0.7027 - val_accuracy: 53.5354%\nEpoch 64/400\n26/26 - train_loss: 1.2268 - train_accuracy: 58.4010%                 - val_loss: 0.7033 - val_accuracy: 53.0303%\nEpoch 65/400\n26/26 - train_loss: 1.2536 - train_accuracy: 58.9007%                 - val_loss: 0.6865 - val_accuracy: 53.0303%\nEpoch 66/400\n26/26 - train_loss: 1.2378 - train_accuracy: 60.3373%                 - val_loss: 0.6622 - val_accuracy: 56.0606%\nEpoch 67/400\n26/26 - train_loss: 1.2014 - train_accuracy: 60.7121%                 - val_loss: 0.6678 - val_accuracy: 55.0505%\nEpoch 68/400\n26/26 - train_loss: 1.2252 - train_accuracy: 59.3379%                 - val_loss: 0.6724 - val_accuracy: 54.5455%\nEpoch 69/400\n26/26 - train_loss: 1.2472 - train_accuracy: 60.5247%                 - val_loss: 0.6699 - val_accuracy: 55.0505%\nEpoch 70/400\n26/26 - train_loss: 1.2021 - train_accuracy: 60.7745%                 - val_loss: 0.6637 - val_accuracy: 56.5657%\nEpoch 71/400\n26/26 - train_loss: 1.2577 - train_accuracy: 62.0862%                 - val_loss: 0.6595 - val_accuracy: 57.0707%\nEpoch 72/400\n26/26 - train_loss: 1.1976 - train_accuracy: 62.1487%                 - val_loss: 0.6566 - val_accuracy: 57.5758%\nEpoch 73/400\n26/26 - train_loss: 1.1911 - train_accuracy: 61.5865%                 - val_loss: 0.6581 - val_accuracy: 59.5960%\nEpoch 74/400\n26/26 - train_loss: 1.1798 - train_accuracy: 61.8364%                 - val_loss: 0.6518 - val_accuracy: 59.5960%\nEpoch 75/400\n26/26 - train_loss: 1.1650 - train_accuracy: 63.5228%                 - val_loss: 0.6448 - val_accuracy: 58.0808%\nEpoch 76/400\n26/26 - train_loss: 1.1776 - train_accuracy: 62.1487%                 - val_loss: 0.6416 - val_accuracy: 60.6061%\nEpoch 77/400\n26/26 - train_loss: 1.1645 - train_accuracy: 63.8976%                 - val_loss: 0.6384 - val_accuracy: 60.6061%\nEpoch 78/400\n26/26 - train_loss: 1.1548 - train_accuracy: 64.0849%                 - val_loss: 0.6357 - val_accuracy: 61.1111%\nEpoch 79/400\n26/26 - train_loss: 1.1374 - train_accuracy: 64.9594%                 - val_loss: 0.6340 - val_accuracy: 61.6162%\nEpoch 80/400\n26/26 - train_loss: 1.1473 - train_accuracy: 64.0849%                 - val_loss: 0.6401 - val_accuracy: 60.1010%\nEpoch 81/400\n26/26 - train_loss: 1.1354 - train_accuracy: 64.8345%                 - val_loss: 0.6312 - val_accuracy: 60.1010%\nEpoch 82/400\n26/26 - train_loss: 1.1561 - train_accuracy: 65.5840%                 - val_loss: 0.6251 - val_accuracy: 61.6162%\nEpoch 83/400\n26/26 - train_loss: 1.1165 - train_accuracy: 65.9588%                 - val_loss: 0.6193 - val_accuracy: 62.6263%\nEpoch 84/400\n26/26 - train_loss: 1.0901 - train_accuracy: 66.3335%                 - val_loss: 0.6148 - val_accuracy: 62.6263%\nEpoch 85/400\n26/26 - train_loss: 1.1009 - train_accuracy: 66.7708%                 - val_loss: 0.6172 - val_accuracy: 61.6162%\nEpoch 86/400\n26/26 - train_loss: 1.0914 - train_accuracy: 66.3335%                 - val_loss: 0.6153 - val_accuracy: 62.1212%\nEpoch 87/400\n26/26 - train_loss: 1.0733 - train_accuracy: 67.2080%                 - val_loss: 0.6017 - val_accuracy: 62.1212%\nEpoch 88/400\n26/26 - train_loss: 1.1087 - train_accuracy: 66.2086%                 - val_loss: 0.6012 - val_accuracy: 62.6263%\nEpoch 89/400\n26/26 - train_loss: 1.0838 - train_accuracy: 66.4585%                 - val_loss: 0.5939 - val_accuracy: 61.6162%\nEpoch 90/400\n26/26 - train_loss: 1.0725 - train_accuracy: 67.2705%                 - val_loss: 0.5895 - val_accuracy: 61.6162%\nEpoch 91/400\n26/26 - train_loss: 1.1398 - train_accuracy: 67.4578%                 - val_loss: 0.5892 - val_accuracy: 62.6263%\nEpoch 92/400\n26/26 - train_loss: 1.0722 - train_accuracy: 66.9582%                 - val_loss: 0.5818 - val_accuracy: 62.6263%\nEpoch 93/400\n26/26 - train_loss: 1.0959 - train_accuracy: 68.0824%                 - val_loss: 0.5820 - val_accuracy: 63.6364%\nEpoch 94/400\n26/26 - train_loss: 1.0501 - train_accuracy: 67.8951%                 - val_loss: 0.5774 - val_accuracy: 63.6364%\nEpoch 95/400\n26/26 - train_loss: 1.0513 - train_accuracy: 68.0200%                 - val_loss: 0.5719 - val_accuracy: 64.6465%\nEpoch 96/400\n26/26 - train_loss: 1.0571 - train_accuracy: 67.0831%                 - val_loss: 0.5683 - val_accuracy: 64.1414%\nEpoch 97/400\n26/26 - train_loss: 1.0468 - train_accuracy: 68.3948%                 - val_loss: 0.5755 - val_accuracy: 63.6364%\nEpoch 98/400\n26/26 - train_loss: 1.0349 - train_accuracy: 69.5815%                 - val_loss: 0.5795 - val_accuracy: 64.1414%\nEpoch 99/400\n26/26 - train_loss: 1.0628 - train_accuracy: 69.8314%                 - val_loss: 0.5845 - val_accuracy: 63.6364%\nEpoch 100/400\n26/26 - train_loss: 1.0169 - train_accuracy: 69.3941%                 - val_loss: 0.5911 - val_accuracy: 64.1414%\nEpoch 101/400\n26/26 - train_loss: 1.0193 - train_accuracy: 70.7058%                 - val_loss: 0.5904 - val_accuracy: 64.1414%\nEpoch 102/400\n26/26 - train_loss: 1.0047 - train_accuracy: 70.3935%                 - val_loss: 0.5995 - val_accuracy: 64.6465%\nEpoch 103/400\n26/26 - train_loss: 1.0324 - train_accuracy: 70.0187%                 - val_loss: 0.5989 - val_accuracy: 64.6465%\nEpoch 104/400\n26/26 - train_loss: 1.0078 - train_accuracy: 70.7683%                 - val_loss: 0.5907 - val_accuracy: 65.1515%\nEpoch 105/400\n26/26 - train_loss: 0.9979 - train_accuracy: 69.6440%                 - val_loss: 0.5826 - val_accuracy: 65.6566%\nEpoch 106/400\n26/26 - train_loss: 1.0276 - train_accuracy: 71.3304%                 - val_loss: 0.5736 - val_accuracy: 65.6566%\nEpoch 107/400\n26/26 - train_loss: 0.9781 - train_accuracy: 70.8307%                 - val_loss: 0.5584 - val_accuracy: 65.6566%\nEpoch 108/400\n26/26 - train_loss: 0.9793 - train_accuracy: 71.0181%                 - val_loss: 0.5555 - val_accuracy: 65.6566%\nEpoch 109/400\n26/26 - train_loss: 0.9787 - train_accuracy: 72.3923%                 - val_loss: 0.5600 - val_accuracy: 64.1414%\nEpoch 110/400\n26/26 - train_loss: 0.9620 - train_accuracy: 71.5803%                 - val_loss: 0.5667 - val_accuracy: 64.6465%\nEpoch 111/400\n26/26 - train_loss: 0.9464 - train_accuracy: 72.0800%                 - val_loss: 0.5659 - val_accuracy: 65.1515%\nEpoch 112/400\n26/26 - train_loss: 0.9779 - train_accuracy: 72.4547%                 - val_loss: 0.5668 - val_accuracy: 66.6667%\nEpoch 113/400\n26/26 - train_loss: 0.9347 - train_accuracy: 72.3298%                 - val_loss: 0.5660 - val_accuracy: 66.6667%\nEpoch 114/400\n26/26 - train_loss: 0.9265 - train_accuracy: 73.0169%                 - val_loss: 0.5617 - val_accuracy: 66.6667%\nEpoch 115/400\n26/26 - train_loss: 0.9578 - train_accuracy: 72.0800%                 - val_loss: 0.5592 - val_accuracy: 66.6667%\nEpoch 116/400\n26/26 - train_loss: 0.9247 - train_accuracy: 72.9544%                 - val_loss: 0.5648 - val_accuracy: 66.1616%\nEpoch 117/400\n26/26 - train_loss: 0.9145 - train_accuracy: 71.8926%                 - val_loss: 0.5621 - val_accuracy: 66.6667%\nEpoch 118/400\n26/26 - train_loss: 0.9075 - train_accuracy: 72.2673%                 - val_loss: 0.5548 - val_accuracy: 66.6667%\nEpoch 119/400\n26/26 - train_loss: 0.9178 - train_accuracy: 72.5172%                 - val_loss: 0.5523 - val_accuracy: 67.1717%\nEpoch 120/400\n26/26 - train_loss: 0.9140 - train_accuracy: 72.8919%                 - val_loss: 0.5459 - val_accuracy: 66.6667%\nEpoch 121/400\n26/26 - train_loss: 0.9282 - train_accuracy: 73.8289%                 - val_loss: 0.5528 - val_accuracy: 67.1717%\nEpoch 122/400\n26/26 - train_loss: 0.9129 - train_accuracy: 72.8295%                 - val_loss: 0.5530 - val_accuracy: 68.1818%\nEpoch 123/400\n26/26 - train_loss: 0.8812 - train_accuracy: 74.2036%                 - val_loss: 0.5502 - val_accuracy: 69.6970%\nEpoch 124/400\n26/26 - train_loss: 0.8769 - train_accuracy: 74.4535%                 - val_loss: 0.5473 - val_accuracy: 70.2020%\nEpoch 125/400\n26/26 - train_loss: 0.8767 - train_accuracy: 73.5790%                 - val_loss: 0.5435 - val_accuracy: 69.6970%\nEpoch 126/400\n26/26 - train_loss: 0.8806 - train_accuracy: 73.8913%                 - val_loss: 0.5431 - val_accuracy: 68.1818%\nEpoch 127/400\n26/26 - train_loss: 0.9041 - train_accuracy: 74.0162%                 - val_loss: 0.5361 - val_accuracy: 69.6970%\nEpoch 128/400\n26/26 - train_loss: 0.8627 - train_accuracy: 74.0787%                 - val_loss: 0.5154 - val_accuracy: 71.2121%\nEpoch 129/400\n26/26 - train_loss: 0.9098 - train_accuracy: 73.6415%                 - val_loss: 0.5160 - val_accuracy: 71.2121%\nEpoch 130/400\n26/26 - train_loss: 0.8683 - train_accuracy: 75.1405%                 - val_loss: 0.5080 - val_accuracy: 72.2222%\nEpoch 131/400\n26/26 - train_loss: 0.8826 - train_accuracy: 75.5778%                 - val_loss: 0.5141 - val_accuracy: 72.2222%\nEpoch 132/400\n26/26 - train_loss: 0.8585 - train_accuracy: 74.8907%                 - val_loss: 0.5035 - val_accuracy: 72.7273%\nEpoch 133/400\n26/26 - train_loss: 0.8861 - train_accuracy: 75.1405%                 - val_loss: 0.5046 - val_accuracy: 72.7273%\nEpoch 134/400\n26/26 - train_loss: 0.8426 - train_accuracy: 74.7658%                 - val_loss: 0.5116 - val_accuracy: 73.2323%\nEpoch 135/400\n26/26 - train_loss: 0.8818 - train_accuracy: 76.0775%                 - val_loss: 0.5113 - val_accuracy: 73.2323%\nEpoch 136/400\n26/26 - train_loss: 0.8457 - train_accuracy: 75.4528%                 - val_loss: 0.5073 - val_accuracy: 72.2222%\nEpoch 137/400\n26/26 - train_loss: 0.8350 - train_accuracy: 76.1399%                 - val_loss: 0.5137 - val_accuracy: 72.7273%\nEpoch 138/400\n26/26 - train_loss: 0.8234 - train_accuracy: 75.5778%                 - val_loss: 0.5085 - val_accuracy: 73.2323%\nEpoch 139/400\n26/26 - train_loss: 0.8382 - train_accuracy: 75.6402%                 - val_loss: 0.5080 - val_accuracy: 73.2323%\nEpoch 140/400\n26/26 - train_loss: 0.8180 - train_accuracy: 75.9525%                 - val_loss: 0.5057 - val_accuracy: 73.2323%\nEpoch 141/400\n26/26 - train_loss: 0.8505 - train_accuracy: 76.5771%                 - val_loss: 0.5078 - val_accuracy: 73.2323%\nEpoch 142/400\n26/26 - train_loss: 0.8057 - train_accuracy: 76.3898%                 - val_loss: 0.5141 - val_accuracy: 74.2424%\nEpoch 143/400\n26/26 - train_loss: 0.7928 - train_accuracy: 76.3273%                 - val_loss: 0.5152 - val_accuracy: 73.7374%\nEpoch 144/400\n26/26 - train_loss: 0.7992 - train_accuracy: 76.5771%                 - val_loss: 0.5123 - val_accuracy: 74.2424%\nEpoch 145/400\n26/26 - train_loss: 0.8018 - train_accuracy: 76.9519%                 - val_loss: 0.5125 - val_accuracy: 74.2424%\nEpoch 146/400\n26/26 - train_loss: 0.7782 - train_accuracy: 77.3267%                 - val_loss: 0.5063 - val_accuracy: 74.2424%\nEpoch 147/400\n26/26 - train_loss: 0.7908 - train_accuracy: 77.6390%                 - val_loss: 0.5053 - val_accuracy: 73.7374%\nEpoch 148/400\n26/26 - train_loss: 0.7800 - train_accuracy: 77.0144%                 - val_loss: 0.5009 - val_accuracy: 75.2525%\nEpoch 149/400\n26/26 - train_loss: 0.7692 - train_accuracy: 76.8270%                 - val_loss: 0.4976 - val_accuracy: 75.2525%\nEpoch 150/400\n26/26 - train_loss: 0.7940 - train_accuracy: 78.2011%                 - val_loss: 0.4887 - val_accuracy: 75.2525%\nEpoch 151/400\n26/26 - train_loss: 0.7886 - train_accuracy: 76.8270%                 - val_loss: 0.4730 - val_accuracy: 75.2525%\nEpoch 152/400\n26/26 - train_loss: 0.7555 - train_accuracy: 77.4516%                 - val_loss: 0.4777 - val_accuracy: 75.2525%\nEpoch 153/400\n26/26 - train_loss: 0.7598 - train_accuracy: 77.3267%                 - val_loss: 0.4695 - val_accuracy: 75.2525%\nEpoch 154/400\n26/26 - train_loss: 0.7763 - train_accuracy: 77.5765%                 - val_loss: 0.4628 - val_accuracy: 75.7576%\nEpoch 155/400\n26/26 - train_loss: 0.7379 - train_accuracy: 77.3267%                 - val_loss: 0.4555 - val_accuracy: 74.7475%\nEpoch 156/400\n26/26 - train_loss: 0.7394 - train_accuracy: 78.8257%                 - val_loss: 0.4568 - val_accuracy: 74.7475%\nEpoch 157/400\n26/26 - train_loss: 0.7502 - train_accuracy: 79.6377%                 - val_loss: 0.4560 - val_accuracy: 74.7475%\nEpoch 158/400\n26/26 - train_loss: 0.7702 - train_accuracy: 76.5147%                 - val_loss: 0.4736 - val_accuracy: 74.2424%\nEpoch 159/400\n26/26 - train_loss: 0.7224 - train_accuracy: 78.3885%                 - val_loss: 0.4662 - val_accuracy: 73.7374%\nEpoch 160/400\n26/26 - train_loss: 0.7450 - train_accuracy: 78.1387%                 - val_loss: 0.4515 - val_accuracy: 74.7475%\nEpoch 161/400\n26/26 - train_loss: 0.7297 - train_accuracy: 78.5759%                 - val_loss: 0.4432 - val_accuracy: 76.2626%\nEpoch 162/400\n26/26 - train_loss: 0.7493 - train_accuracy: 78.5759%                 - val_loss: 0.4426 - val_accuracy: 76.7677%\nEpoch 163/400\n26/26 - train_loss: 0.7036 - train_accuracy: 77.5765%                 - val_loss: 0.4475 - val_accuracy: 76.7677%\nEpoch 164/400\n26/26 - train_loss: 0.7395 - train_accuracy: 79.2005%                 - val_loss: 0.4460 - val_accuracy: 77.2727%\nEpoch 165/400\n26/26 - train_loss: 0.6893 - train_accuracy: 78.6384%                 - val_loss: 0.4336 - val_accuracy: 77.7778%\nEpoch 166/400\n26/26 - train_loss: 0.7270 - train_accuracy: 80.5746%                 - val_loss: 0.4298 - val_accuracy: 77.7778%\nEpoch 167/400\n26/26 - train_loss: 0.6894 - train_accuracy: 78.9507%                 - val_loss: 0.4345 - val_accuracy: 77.2727%\nEpoch 168/400\n26/26 - train_loss: 0.6937 - train_accuracy: 79.2005%                 - val_loss: 0.4336 - val_accuracy: 77.2727%\nEpoch 169/400\n26/26 - train_loss: 0.6967 - train_accuracy: 78.9507%                 - val_loss: 0.4336 - val_accuracy: 76.7677%\nEpoch 170/400\n26/26 - train_loss: 0.7050 - train_accuracy: 78.4510%                 - val_loss: 0.4397 - val_accuracy: 76.7677%\nEpoch 171/400\n26/26 - train_loss: 0.7075 - train_accuracy: 79.5128%                 - val_loss: 0.4382 - val_accuracy: 77.2727%\nEpoch 172/400\n26/26 - train_loss: 0.7273 - train_accuracy: 79.2630%                 - val_loss: 0.4045 - val_accuracy: 79.2929%\nEpoch 173/400\n26/26 - train_loss: 0.6812 - train_accuracy: 79.4503%                 - val_loss: 0.4083 - val_accuracy: 79.7980%\nEpoch 174/400\n26/26 - train_loss: 0.6715 - train_accuracy: 78.7633%                 - val_loss: 0.4061 - val_accuracy: 79.7980%\nEpoch 175/400\n26/26 - train_loss: 0.6803 - train_accuracy: 79.3879%                 - val_loss: 0.4020 - val_accuracy: 79.7980%\nEpoch 176/400\n26/26 - train_loss: 0.6887 - train_accuracy: 80.1999%                 - val_loss: 0.4035 - val_accuracy: 79.2929%\nEpoch 177/400\n26/26 - train_loss: 0.7133 - train_accuracy: 80.3248%                 - val_loss: 0.3960 - val_accuracy: 79.7980%\nEpoch 178/400\n26/26 - train_loss: 0.6787 - train_accuracy: 81.1368%                 - val_loss: 0.3833 - val_accuracy: 80.3030%\nEpoch 179/400\n26/26 - train_loss: 0.6443 - train_accuracy: 80.4497%                 - val_loss: 0.3680 - val_accuracy: 80.8081%\nEpoch 180/400\n26/26 - train_loss: 0.6500 - train_accuracy: 81.1368%                 - val_loss: 0.3706 - val_accuracy: 80.8081%\nEpoch 181/400\n26/26 - train_loss: 0.6917 - train_accuracy: 79.2005%                 - val_loss: 0.3715 - val_accuracy: 80.8081%\nEpoch 182/400\n26/26 - train_loss: 0.6510 - train_accuracy: 80.8869%                 - val_loss: 0.3812 - val_accuracy: 80.8081%\nEpoch 183/400\n26/26 - train_loss: 0.6336 - train_accuracy: 80.7620%                 - val_loss: 0.3795 - val_accuracy: 80.8081%\nEpoch 184/400\n26/26 - train_loss: 0.6404 - train_accuracy: 81.3242%                 - val_loss: 0.3715 - val_accuracy: 81.3131%\nEpoch 185/400\n26/26 - train_loss: 0.6453 - train_accuracy: 81.9488%                 - val_loss: 0.3640 - val_accuracy: 81.3131%\nEpoch 186/400\n26/26 - train_loss: 0.6466 - train_accuracy: 82.6359%                 - val_loss: 0.3622 - val_accuracy: 81.3131%\nEpoch 187/400\n26/26 - train_loss: 0.6620 - train_accuracy: 81.6365%                 - val_loss: 0.3609 - val_accuracy: 81.8182%\nEpoch 188/400\n26/26 - train_loss: 0.6319 - train_accuracy: 81.1993%                 - val_loss: 0.3642 - val_accuracy: 82.3232%\nEpoch 189/400\n26/26 - train_loss: 0.6826 - train_accuracy: 81.4491%                 - val_loss: 0.3549 - val_accuracy: 82.8283%\nEpoch 190/400\n26/26 - train_loss: 0.6976 - train_accuracy: 83.7601%                 - val_loss: 0.3522 - val_accuracy: 82.3232%\nEpoch 191/400\n26/26 - train_loss: 0.6101 - train_accuracy: 82.5734%                 - val_loss: 0.3666 - val_accuracy: 82.3232%\nEpoch 192/400\n26/26 - train_loss: 0.6029 - train_accuracy: 83.2605%                 - val_loss: 0.3617 - val_accuracy: 82.3232%\nEpoch 193/400\n26/26 - train_loss: 0.6301 - train_accuracy: 82.6359%                 - val_loss: 0.3616 - val_accuracy: 82.3232%\nEpoch 194/400\n26/26 - train_loss: 0.6259 - train_accuracy: 82.1986%                 - val_loss: 0.3661 - val_accuracy: 82.3232%\nEpoch 195/400\n26/26 - train_loss: 0.6066 - train_accuracy: 82.5734%                 - val_loss: 0.3633 - val_accuracy: 81.8182%\nEpoch 196/400\n26/26 - train_loss: 0.5999 - train_accuracy: 82.3235%                 - val_loss: 0.3564 - val_accuracy: 81.8182%\nEpoch 197/400\n26/26 - train_loss: 0.5869 - train_accuracy: 82.7608%                 - val_loss: 0.3492 - val_accuracy: 81.8182%\nEpoch 198/400\n26/26 - train_loss: 0.5838 - train_accuracy: 82.8857%                 - val_loss: 0.3401 - val_accuracy: 81.8182%\nEpoch 199/400\n26/26 - train_loss: 0.6153 - train_accuracy: 84.0100%                 - val_loss: 0.3335 - val_accuracy: 81.8182%\nEpoch 200/400\n26/26 - train_loss: 0.6318 - train_accuracy: 82.5734%                 - val_loss: 0.3366 - val_accuracy: 81.8182%\nEpoch 201/400\n26/26 - train_loss: 0.5861 - train_accuracy: 83.2605%                 - val_loss: 0.3314 - val_accuracy: 81.3131%\nEpoch 202/400\n26/26 - train_loss: 0.5704 - train_accuracy: 84.0100%                 - val_loss: 0.3292 - val_accuracy: 81.3131%\nEpoch 203/400\n26/26 - train_loss: 0.5801 - train_accuracy: 83.0731%                 - val_loss: 0.3249 - val_accuracy: 81.8182%\nEpoch 204/400\n26/26 - train_loss: 0.5721 - train_accuracy: 84.1349%                 - val_loss: 0.3211 - val_accuracy: 81.8182%\nEpoch 205/400\n26/26 - train_loss: 0.6046 - train_accuracy: 83.4478%                 - val_loss: 0.3192 - val_accuracy: 84.3434%\nEpoch 206/400\n26/26 - train_loss: 0.5651 - train_accuracy: 84.5097%                 - val_loss: 0.3297 - val_accuracy: 82.8283%\nEpoch 207/400\n26/26 - train_loss: 0.5710 - train_accuracy: 84.1349%                 - val_loss: 0.3202 - val_accuracy: 83.8384%\nEpoch 208/400\n26/26 - train_loss: 0.5833 - train_accuracy: 82.7608%                 - val_loss: 0.3194 - val_accuracy: 82.8283%\nEpoch 209/400\n26/26 - train_loss: 0.5954 - train_accuracy: 84.4472%                 - val_loss: 0.3025 - val_accuracy: 82.8283%\nEpoch 210/400\n26/26 - train_loss: 0.5426 - train_accuracy: 83.9475%                 - val_loss: 0.2921 - val_accuracy: 83.8384%\nEpoch 211/400\n26/26 - train_loss: 0.6049 - train_accuracy: 84.5721%                 - val_loss: 0.2952 - val_accuracy: 84.3434%\nEpoch 212/400\n26/26 - train_loss: 0.5586 - train_accuracy: 84.9469%                 - val_loss: 0.2982 - val_accuracy: 84.3434%\nEpoch 213/400\n26/26 - train_loss: 0.5325 - train_accuracy: 84.4472%                 - val_loss: 0.2928 - val_accuracy: 83.8384%\nEpoch 214/400\n26/26 - train_loss: 0.5589 - train_accuracy: 84.7595%                 - val_loss: 0.2891 - val_accuracy: 83.8384%\nEpoch 215/400\n26/26 - train_loss: 0.5503 - train_accuracy: 84.4472%                 - val_loss: 0.2921 - val_accuracy: 83.3333%\nEpoch 216/400\n26/26 - train_loss: 0.5418 - train_accuracy: 85.8838%                 - val_loss: 0.2847 - val_accuracy: 83.3333%\nEpoch 217/400\n26/26 - train_loss: 0.5486 - train_accuracy: 86.0712%                 - val_loss: 0.2810 - val_accuracy: 83.8384%\nEpoch 218/400\n26/26 - train_loss: 0.5291 - train_accuracy: 84.6346%                 - val_loss: 0.2800 - val_accuracy: 83.8384%\nEpoch 219/400\n26/26 - train_loss: 0.5264 - train_accuracy: 84.9469%                 - val_loss: 0.2783 - val_accuracy: 83.8384%\nEpoch 220/400\n26/26 - train_loss: 0.5268 - train_accuracy: 85.8838%                 - val_loss: 0.2746 - val_accuracy: 83.8384%\nEpoch 221/400\n26/26 - train_loss: 0.5439 - train_accuracy: 84.3223%                 - val_loss: 0.2674 - val_accuracy: 83.8384%\nEpoch 222/400\n26/26 - train_loss: 0.5484 - train_accuracy: 84.9469%                 - val_loss: 0.2673 - val_accuracy: 83.8384%\nEpoch 223/400\n26/26 - train_loss: 0.5233 - train_accuracy: 85.3841%                 - val_loss: 0.2587 - val_accuracy: 84.3434%\nEpoch 224/400\n26/26 - train_loss: 0.5102 - train_accuracy: 85.4466%                 - val_loss: 0.2575 - val_accuracy: 83.8384%\nEpoch 225/400\n26/26 - train_loss: 0.5353 - train_accuracy: 85.1343%                 - val_loss: 0.2550 - val_accuracy: 84.3434%\nEpoch 226/400\n26/26 - train_loss: 0.5020 - train_accuracy: 85.0094%                 - val_loss: 0.2528 - val_accuracy: 84.8485%\nEpoch 227/400\n26/26 - train_loss: 0.5831 - train_accuracy: 85.4466%                 - val_loss: 0.2517 - val_accuracy: 85.3535%\nEpoch 228/400\n26/26 - train_loss: 0.5037 - train_accuracy: 86.2586%                 - val_loss: 0.2626 - val_accuracy: 84.8485%\nEpoch 229/400\n26/26 - train_loss: 0.5104 - train_accuracy: 86.8832%                 - val_loss: 0.2577 - val_accuracy: 85.3535%\nEpoch 230/400\n26/26 - train_loss: 0.4926 - train_accuracy: 86.3835%                 - val_loss: 0.2551 - val_accuracy: 85.3535%\nEpoch 231/400\n26/26 - train_loss: 0.5400 - train_accuracy: 86.6334%                 - val_loss: 0.2513 - val_accuracy: 85.3535%\nEpoch 232/400\n26/26 - train_loss: 0.4819 - train_accuracy: 86.5084%                 - val_loss: 0.2447 - val_accuracy: 85.8586%\nEpoch 233/400\n26/26 - train_loss: 0.5267 - train_accuracy: 85.7589%                 - val_loss: 0.2420 - val_accuracy: 85.8586%\nEpoch 234/400\n26/26 - train_loss: 0.4934 - train_accuracy: 85.8838%                 - val_loss: 0.2466 - val_accuracy: 85.8586%\nEpoch 235/400\n26/26 - train_loss: 0.4900 - train_accuracy: 85.3841%                 - val_loss: 0.2430 - val_accuracy: 85.8586%\nEpoch 236/400\n26/26 - train_loss: 0.5185 - train_accuracy: 86.8832%                 - val_loss: 0.2389 - val_accuracy: 85.8586%\nEpoch 237/400\n26/26 - train_loss: 0.4734 - train_accuracy: 86.3835%                 - val_loss: 0.2395 - val_accuracy: 85.8586%\nEpoch 238/400\n26/26 - train_loss: 0.4833 - train_accuracy: 86.8207%                 - val_loss: 0.2333 - val_accuracy: 86.3636%\nEpoch 239/400\n26/26 - train_loss: 0.4766 - train_accuracy: 86.5709%                 - val_loss: 0.2287 - val_accuracy: 86.3636%\nEpoch 240/400\n26/26 - train_loss: 0.4771 - train_accuracy: 86.6958%                 - val_loss: 0.2258 - val_accuracy: 86.8687%\nEpoch 241/400\n26/26 - train_loss: 0.5000 - train_accuracy: 86.5084%                 - val_loss: 0.2223 - val_accuracy: 86.8687%\nEpoch 242/400\n26/26 - train_loss: 0.4738 - train_accuracy: 86.6334%                 - val_loss: 0.2263 - val_accuracy: 86.8687%\nEpoch 243/400\n26/26 - train_loss: 0.4967 - train_accuracy: 86.0712%                 - val_loss: 0.2241 - val_accuracy: 86.8687%\nEpoch 244/400\n26/26 - train_loss: 0.4941 - train_accuracy: 85.8214%                 - val_loss: 0.2174 - val_accuracy: 86.8687%\nEpoch 245/400\n26/26 - train_loss: 0.4529 - train_accuracy: 88.1949%                 - val_loss: 0.2162 - val_accuracy: 86.8687%\nEpoch 246/400\n26/26 - train_loss: 0.5251 - train_accuracy: 87.6952%                 - val_loss: 0.2125 - val_accuracy: 86.8687%\nEpoch 247/400\n26/26 - train_loss: 0.4562 - train_accuracy: 87.0706%                 - val_loss: 0.2130 - val_accuracy: 86.8687%\nEpoch 248/400\n26/26 - train_loss: 0.4526 - train_accuracy: 86.7583%                 - val_loss: 0.2093 - val_accuracy: 87.3737%\nEpoch 249/400\n26/26 - train_loss: 0.4562 - train_accuracy: 88.0075%                 - val_loss: 0.2063 - val_accuracy: 87.3737%\nEpoch 250/400\n26/26 - train_loss: 0.4696 - train_accuracy: 88.2573%                 - val_loss: 0.2005 - val_accuracy: 87.3737%\nEpoch 251/400\n26/26 - train_loss: 0.4736 - train_accuracy: 87.6327%                 - val_loss: 0.1961 - val_accuracy: 87.8788%\nEpoch 252/400\n26/26 - train_loss: 0.4718 - train_accuracy: 88.5696%                 - val_loss: 0.2010 - val_accuracy: 87.3737%\nEpoch 253/400\n26/26 - train_loss: 0.4476 - train_accuracy: 87.1955%                 - val_loss: 0.2012 - val_accuracy: 87.3737%\nEpoch 254/400\n26/26 - train_loss: 0.4349 - train_accuracy: 88.1949%                 - val_loss: 0.1980 - val_accuracy: 87.3737%\nEpoch 255/400\n26/26 - train_loss: 0.4316 - train_accuracy: 88.1324%                 - val_loss: 0.1960 - val_accuracy: 87.3737%\nEpoch 256/400\n26/26 - train_loss: 0.4281 - train_accuracy: 87.5703%                 - val_loss: 0.1902 - val_accuracy: 87.8788%\nEpoch 257/400\n26/26 - train_loss: 0.4353 - train_accuracy: 88.5696%                 - val_loss: 0.1870 - val_accuracy: 87.8788%\nEpoch 258/400\n26/26 - train_loss: 0.4364 - train_accuracy: 88.1324%                 - val_loss: 0.1856 - val_accuracy: 87.8788%\nEpoch 259/400\n26/26 - train_loss: 0.4298 - train_accuracy: 88.3198%                 - val_loss: 0.1832 - val_accuracy: 88.8889%\nEpoch 260/400\n26/26 - train_loss: 0.4245 - train_accuracy: 88.2573%                 - val_loss: 0.1814 - val_accuracy: 88.3838%\nEpoch 261/400\n26/26 - train_loss: 0.4240 - train_accuracy: 87.8201%                 - val_loss: 0.1788 - val_accuracy: 89.3939%\nEpoch 262/400\n26/26 - train_loss: 0.4285 - train_accuracy: 89.0693%                 - val_loss: 0.1771 - val_accuracy: 88.8889%\nEpoch 263/400\n26/26 - train_loss: 0.4156 - train_accuracy: 88.3198%                 - val_loss: 0.1793 - val_accuracy: 89.8990%\nEpoch 264/400\n26/26 - train_loss: 0.4260 - train_accuracy: 88.7570%                 - val_loss: 0.1754 - val_accuracy: 89.3939%\nEpoch 265/400\n26/26 - train_loss: 0.4083 - train_accuracy: 88.7570%                 - val_loss: 0.1746 - val_accuracy: 90.4040%\nEpoch 266/400\n26/26 - train_loss: 0.4248 - train_accuracy: 88.7570%                 - val_loss: 0.1704 - val_accuracy: 90.9091%\nEpoch 267/400\n26/26 - train_loss: 0.4735 - train_accuracy: 88.6321%                 - val_loss: 0.1673 - val_accuracy: 90.9091%\nEpoch 268/400\n26/26 - train_loss: 0.4357 - train_accuracy: 89.1318%                 - val_loss: 0.1685 - val_accuracy: 90.9091%\nEpoch 269/400\n26/26 - train_loss: 0.4167 - train_accuracy: 88.9444%                 - val_loss: 0.1645 - val_accuracy: 90.9091%\nEpoch 270/400\n26/26 - train_loss: 0.4022 - train_accuracy: 89.8189%                 - val_loss: 0.1632 - val_accuracy: 90.9091%\nEpoch 271/400\n26/26 - train_loss: 0.4920 - train_accuracy: 89.8813%                 - val_loss: 0.1607 - val_accuracy: 90.9091%\nEpoch 272/400\n26/26 - train_loss: 0.4138 - train_accuracy: 89.8189%                 - val_loss: 0.1628 - val_accuracy: 90.4040%\nEpoch 273/400\n26/26 - train_loss: 0.4228 - train_accuracy: 89.0069%                 - val_loss: 0.1603 - val_accuracy: 90.9091%\nEpoch 274/400\n26/26 - train_loss: 0.3890 - train_accuracy: 89.7564%                 - val_loss: 0.1586 - val_accuracy: 90.9091%\nEpoch 275/400\n26/26 - train_loss: 0.3969 - train_accuracy: 88.9444%                 - val_loss: 0.1564 - val_accuracy: 90.9091%\nEpoch 276/400\n26/26 - train_loss: 0.3877 - train_accuracy: 89.3816%                 - val_loss: 0.1529 - val_accuracy: 90.4040%\nEpoch 277/400\n26/26 - train_loss: 0.3817 - train_accuracy: 89.6939%                 - val_loss: 0.1503 - val_accuracy: 90.9091%\nEpoch 278/400\n26/26 - train_loss: 0.4069 - train_accuracy: 89.7564%                 - val_loss: 0.1499 - val_accuracy: 90.9091%\nEpoch 279/400\n26/26 - train_loss: 0.3830 - train_accuracy: 89.8189%                 - val_loss: 0.1527 - val_accuracy: 91.4141%\nEpoch 280/400\n26/26 - train_loss: 0.3767 - train_accuracy: 90.2561%                 - val_loss: 0.1496 - val_accuracy: 91.4141%\nEpoch 281/400\n26/26 - train_loss: 0.3742 - train_accuracy: 90.1936%                 - val_loss: 0.1467 - val_accuracy: 91.4141%\nEpoch 282/400\n26/26 - train_loss: 0.3895 - train_accuracy: 89.5690%                 - val_loss: 0.1437 - val_accuracy: 91.4141%\nEpoch 283/400\n26/26 - train_loss: 0.3680 - train_accuracy: 90.5684%                 - val_loss: 0.1440 - val_accuracy: 91.4141%\nEpoch 284/400\n26/26 - train_loss: 0.3885 - train_accuracy: 89.6315%                 - val_loss: 0.1415 - val_accuracy: 91.4141%\nEpoch 285/400\n26/26 - train_loss: 0.3793 - train_accuracy: 90.4435%                 - val_loss: 0.1382 - val_accuracy: 90.9091%\nEpoch 286/400\n26/26 - train_loss: 0.3822 - train_accuracy: 90.3810%                 - val_loss: 0.1357 - val_accuracy: 90.9091%\nEpoch 287/400\n26/26 - train_loss: 0.3646 - train_accuracy: 90.6933%                 - val_loss: 0.1309 - val_accuracy: 90.9091%\nEpoch 288/400\n26/26 - train_loss: 0.3746 - train_accuracy: 90.3186%                 - val_loss: 0.1305 - val_accuracy: 90.9091%\nEpoch 289/400\n26/26 - train_loss: 0.3576 - train_accuracy: 90.2561%                 - val_loss: 0.1298 - val_accuracy: 91.4141%\nEpoch 290/400\n26/26 - train_loss: 0.4087 - train_accuracy: 89.6939%                 - val_loss: 0.1301 - val_accuracy: 91.4141%\nEpoch 291/400\n26/26 - train_loss: 0.3650 - train_accuracy: 90.8182%                 - val_loss: 0.1323 - val_accuracy: 91.9192%\nEpoch 292/400\n26/26 - train_loss: 0.3622 - train_accuracy: 90.7558%                 - val_loss: 0.1304 - val_accuracy: 91.9192%\nEpoch 293/400\n26/26 - train_loss: 0.4662 - train_accuracy: 90.6933%                 - val_loss: 0.1259 - val_accuracy: 91.9192%\nEpoch 294/400\n26/26 - train_loss: 0.3598 - train_accuracy: 90.9432%                 - val_loss: 0.1206 - val_accuracy: 92.4242%\nEpoch 295/400\n26/26 - train_loss: 0.3457 - train_accuracy: 91.0056%                 - val_loss: 0.1212 - val_accuracy: 91.9192%\nEpoch 296/400\n26/26 - train_loss: 0.3669 - train_accuracy: 91.1930%                 - val_loss: 0.1200 - val_accuracy: 91.9192%\nEpoch 297/400\n26/26 - train_loss: 0.3689 - train_accuracy: 91.1930%                 - val_loss: 0.1163 - val_accuracy: 92.4242%\nEpoch 298/400\n26/26 - train_loss: 0.3489 - train_accuracy: 90.4435%                 - val_loss: 0.1152 - val_accuracy: 92.4242%\nEpoch 299/400\n26/26 - train_loss: 0.3514 - train_accuracy: 90.4435%                 - val_loss: 0.1153 - val_accuracy: 92.4242%\nEpoch 300/400\n26/26 - train_loss: 0.4296 - train_accuracy: 90.3186%                 - val_loss: 0.1164 - val_accuracy: 92.4242%\nEpoch 301/400\n26/26 - train_loss: 0.3378 - train_accuracy: 91.5053%                 - val_loss: 0.1248 - val_accuracy: 92.4242%\nEpoch 302/400\n26/26 - train_loss: 0.3441 - train_accuracy: 91.5053%                 - val_loss: 0.1223 - val_accuracy: 92.9293%\nEpoch 303/400\n26/26 - train_loss: 0.3414 - train_accuracy: 91.0056%                 - val_loss: 0.1185 - val_accuracy: 92.9293%\nEpoch 304/400\n26/26 - train_loss: 0.3427 - train_accuracy: 91.0681%                 - val_loss: 0.1146 - val_accuracy: 92.4242%\nEpoch 305/400\n26/26 - train_loss: 0.3365 - train_accuracy: 91.4428%                 - val_loss: 0.1147 - val_accuracy: 92.4242%\nEpoch 306/400\n26/26 - train_loss: 0.3481 - train_accuracy: 91.6927%                 - val_loss: 0.1141 - val_accuracy: 92.4242%\nEpoch 307/400\n26/26 - train_loss: 0.3355 - train_accuracy: 92.2548%                 - val_loss: 0.1145 - val_accuracy: 92.4242%\nEpoch 308/400\n26/26 - train_loss: 0.3308 - train_accuracy: 91.3179%                 - val_loss: 0.1104 - val_accuracy: 92.4242%\nEpoch 309/400\n26/26 - train_loss: 0.3180 - train_accuracy: 91.4428%                 - val_loss: 0.1093 - val_accuracy: 92.4242%\nEpoch 310/400\n26/26 - train_loss: 0.3338 - train_accuracy: 91.3804%                 - val_loss: 0.1069 - val_accuracy: 92.9293%\nEpoch 311/400\n26/26 - train_loss: 0.3274 - train_accuracy: 91.3804%                 - val_loss: 0.1058 - val_accuracy: 92.9293%\nEpoch 312/400\n26/26 - train_loss: 0.3242 - train_accuracy: 91.9425%                 - val_loss: 0.1041 - val_accuracy: 92.9293%\nEpoch 313/400\n26/26 - train_loss: 0.3193 - train_accuracy: 91.8176%                 - val_loss: 0.1029 - val_accuracy: 92.9293%\nEpoch 314/400\n26/26 - train_loss: 0.3196 - train_accuracy: 91.6927%                 - val_loss: 0.1023 - val_accuracy: 92.9293%\nEpoch 315/400\n26/26 - train_loss: 0.3177 - train_accuracy: 92.5047%                 - val_loss: 0.1021 - val_accuracy: 92.9293%\nEpoch 316/400\n26/26 - train_loss: 0.3179 - train_accuracy: 92.1299%                 - val_loss: 0.1016 - val_accuracy: 92.9293%\nEpoch 317/400\n26/26 - train_loss: 0.3202 - train_accuracy: 90.8807%                 - val_loss: 0.1003 - val_accuracy: 92.9293%\nEpoch 318/400\n26/26 - train_loss: 0.3118 - train_accuracy: 92.1924%                 - val_loss: 0.0983 - val_accuracy: 93.4343%\nEpoch 319/400\n26/26 - train_loss: 0.3121 - train_accuracy: 91.4428%                 - val_loss: 0.0974 - val_accuracy: 93.4343%\nEpoch 320/400\n26/26 - train_loss: 0.3035 - train_accuracy: 92.4422%                 - val_loss: 0.0963 - val_accuracy: 93.4343%\nEpoch 321/400\n26/26 - train_loss: 0.3091 - train_accuracy: 92.0050%                 - val_loss: 0.0952 - val_accuracy: 93.4343%\nEpoch 322/400\n26/26 - train_loss: 0.3373 - train_accuracy: 92.8170%                 - val_loss: 0.0950 - val_accuracy: 93.4343%\nEpoch 323/400\n26/26 - train_loss: 0.3092 - train_accuracy: 92.0050%                 - val_loss: 0.0974 - val_accuracy: 92.9293%\nEpoch 324/400\n26/26 - train_loss: 0.3145 - train_accuracy: 91.8176%                 - val_loss: 0.0953 - val_accuracy: 93.4343%\nEpoch 325/400\n26/26 - train_loss: 0.3062 - train_accuracy: 92.6296%                 - val_loss: 0.0923 - val_accuracy: 93.4343%\nEpoch 326/400\n26/26 - train_loss: 0.3203 - train_accuracy: 91.8801%                 - val_loss: 0.0911 - val_accuracy: 93.4343%\nEpoch 327/400\n26/26 - train_loss: 0.3048 - train_accuracy: 92.3173%                 - val_loss: 0.0913 - val_accuracy: 93.4343%\nEpoch 328/400\n26/26 - train_loss: 0.3508 - train_accuracy: 92.3173%                 - val_loss: 0.0894 - val_accuracy: 93.4343%\nEpoch 329/400\n26/26 - train_loss: 0.2890 - train_accuracy: 93.2542%                 - val_loss: 0.0904 - val_accuracy: 93.9394%\nEpoch 330/400\n26/26 - train_loss: 0.3131 - train_accuracy: 92.3173%                 - val_loss: 0.0885 - val_accuracy: 93.9394%\nEpoch 331/400\n26/26 - train_loss: 0.3252 - train_accuracy: 92.2548%                 - val_loss: 0.0853 - val_accuracy: 93.9394%\nEpoch 332/400\n26/26 - train_loss: 0.2934 - train_accuracy: 93.7539%                 - val_loss: 0.0873 - val_accuracy: 93.4343%\nEpoch 333/400\n26/26 - train_loss: 0.2897 - train_accuracy: 92.9419%                 - val_loss: 0.0873 - val_accuracy: 93.4343%\nEpoch 334/400\n26/26 - train_loss: 0.2867 - train_accuracy: 93.0668%                 - val_loss: 0.0857 - val_accuracy: 93.4343%\nEpoch 335/400\n26/26 - train_loss: 0.2888 - train_accuracy: 92.8170%                 - val_loss: 0.0840 - val_accuracy: 93.4343%\nEpoch 336/400\n26/26 - train_loss: 0.2886 - train_accuracy: 93.0668%                 - val_loss: 0.0830 - val_accuracy: 93.4343%\nEpoch 337/400\n26/26 - train_loss: 0.2826 - train_accuracy: 92.6921%                 - val_loss: 0.0822 - val_accuracy: 93.9394%\nEpoch 338/400\n26/26 - train_loss: 0.3191 - train_accuracy: 92.9419%                 - val_loss: 0.0811 - val_accuracy: 93.4343%\nEpoch 339/400\n26/26 - train_loss: 0.3043 - train_accuracy: 92.3798%                 - val_loss: 0.0801 - val_accuracy: 93.4343%\nEpoch 340/400\n26/26 - train_loss: 0.2968 - train_accuracy: 92.7545%                 - val_loss: 0.0777 - val_accuracy: 93.4343%\nEpoch 341/400\n26/26 - train_loss: 0.3299 - train_accuracy: 93.3791%                 - val_loss: 0.0770 - val_accuracy: 93.9394%\nEpoch 342/400\n26/26 - train_loss: 0.2745 - train_accuracy: 93.4416%                 - val_loss: 0.0776 - val_accuracy: 93.4343%\nEpoch 343/400\n26/26 - train_loss: 0.3069 - train_accuracy: 93.4416%                 - val_loss: 0.0761 - val_accuracy: 93.4343%\nEpoch 344/400\n26/26 - train_loss: 0.3043 - train_accuracy: 93.6290%                 - val_loss: 0.0767 - val_accuracy: 93.9394%\nEpoch 345/400\n26/26 - train_loss: 0.2688 - train_accuracy: 93.0044%                 - val_loss: 0.0765 - val_accuracy: 94.4444%\nEpoch 346/400\n26/26 - train_loss: 0.2773 - train_accuracy: 93.1918%                 - val_loss: 0.0757 - val_accuracy: 93.9394%\nEpoch 347/400\n26/26 - train_loss: 0.2691 - train_accuracy: 93.3791%                 - val_loss: 0.0750 - val_accuracy: 93.9394%\nEpoch 348/400\n26/26 - train_loss: 0.2722 - train_accuracy: 93.6914%                 - val_loss: 0.0740 - val_accuracy: 93.9394%\nEpoch 349/400\n26/26 - train_loss: 0.2725 - train_accuracy: 93.1293%                 - val_loss: 0.0734 - val_accuracy: 94.4444%\nEpoch 350/400\n26/26 - train_loss: 0.2651 - train_accuracy: 93.6914%                 - val_loss: 0.0725 - val_accuracy: 94.4444%\nEpoch 351/400\n26/26 - train_loss: 0.2981 - train_accuracy: 92.9419%                 - val_loss: 0.0713 - val_accuracy: 94.4444%\nEpoch 352/400\n26/26 - train_loss: 0.2705 - train_accuracy: 93.0668%                 - val_loss: 0.0666 - val_accuracy: 94.4444%\nEpoch 353/400\n26/26 - train_loss: 0.2704 - train_accuracy: 92.8170%                 - val_loss: 0.0670 - val_accuracy: 94.4444%\nEpoch 354/400\n26/26 - train_loss: 0.2965 - train_accuracy: 93.3791%                 - val_loss: 0.0673 - val_accuracy: 94.4444%\nEpoch 355/400\n26/26 - train_loss: 0.2558 - train_accuracy: 93.9413%                 - val_loss: 0.0671 - val_accuracy: 94.4444%\nEpoch 356/400\n26/26 - train_loss: 0.2638 - train_accuracy: 93.2542%                 - val_loss: 0.0668 - val_accuracy: 94.4444%\nEpoch 357/400\n26/26 - train_loss: 0.2638 - train_accuracy: 93.1293%                 - val_loss: 0.0664 - val_accuracy: 94.4444%\nEpoch 358/400\n26/26 - train_loss: 0.2684 - train_accuracy: 94.2536%                 - val_loss: 0.0662 - val_accuracy: 94.4444%\nEpoch 359/400\n26/26 - train_loss: 0.2604 - train_accuracy: 94.3785%                 - val_loss: 0.0654 - val_accuracy: 94.4444%\nEpoch 360/400\n26/26 - train_loss: 0.2565 - train_accuracy: 94.0037%                 - val_loss: 0.0625 - val_accuracy: 94.4444%\nEpoch 361/400\n26/26 - train_loss: 0.2510 - train_accuracy: 93.6914%                 - val_loss: 0.0627 - val_accuracy: 94.9495%\nEpoch 362/400\n26/26 - train_loss: 0.2667 - train_accuracy: 92.9419%                 - val_loss: 0.0626 - val_accuracy: 94.9495%\nEpoch 363/400\n26/26 - train_loss: 0.2573 - train_accuracy: 93.3791%                 - val_loss: 0.0632 - val_accuracy: 94.9495%\nEpoch 364/400\n26/26 - train_loss: 0.2872 - train_accuracy: 94.3161%                 - val_loss: 0.0632 - val_accuracy: 94.9495%\nEpoch 365/400\n26/26 - train_loss: 0.2529 - train_accuracy: 93.8164%                 - val_loss: 0.0633 - val_accuracy: 95.4545%\nEpoch 366/400\n26/26 - train_loss: 0.2750 - train_accuracy: 93.7539%                 - val_loss: 0.0634 - val_accuracy: 95.4545%\nEpoch 367/400\n26/26 - train_loss: 0.2452 - train_accuracy: 93.8164%                 - val_loss: 0.0647 - val_accuracy: 95.4545%\nEpoch 368/400\n26/26 - train_loss: 0.2615 - train_accuracy: 93.8164%                 - val_loss: 0.0639 - val_accuracy: 94.9495%\nEpoch 369/400\n26/26 - train_loss: 0.2414 - train_accuracy: 94.3785%                 - val_loss: 0.0627 - val_accuracy: 95.4545%\nEpoch 370/400\n26/26 - train_loss: 0.2521 - train_accuracy: 93.6290%                 - val_loss: 0.0616 - val_accuracy: 94.9495%\nEpoch 371/400\n26/26 - train_loss: 0.2505 - train_accuracy: 94.8157%                 - val_loss: 0.0619 - val_accuracy: 95.4545%\nEpoch 372/400\n26/26 - train_loss: 0.2661 - train_accuracy: 94.3785%                 - val_loss: 0.0607 - val_accuracy: 95.4545%\nEpoch 373/400\n26/26 - train_loss: 0.2454 - train_accuracy: 94.4410%                 - val_loss: 0.0598 - val_accuracy: 94.9495%\nEpoch 374/400\n26/26 - train_loss: 0.2418 - train_accuracy: 95.1905%                 - val_loss: 0.0593 - val_accuracy: 94.9495%\nEpoch 375/400\n26/26 - train_loss: 0.2392 - train_accuracy: 94.1287%                 - val_loss: 0.0568 - val_accuracy: 94.9495%\nEpoch 376/400\n26/26 - train_loss: 0.2361 - train_accuracy: 93.8788%                 - val_loss: 0.0555 - val_accuracy: 94.9495%\nEpoch 377/400\n26/26 - train_loss: 0.2330 - train_accuracy: 93.7539%                 - val_loss: 0.0562 - val_accuracy: 94.9495%\nEpoch 378/400\n26/26 - train_loss: 0.2325 - train_accuracy: 94.3161%                 - val_loss: 0.0568 - val_accuracy: 94.9495%\nEpoch 379/400\n26/26 - train_loss: 0.2386 - train_accuracy: 93.4416%                 - val_loss: 0.0568 - val_accuracy: 94.9495%\nEpoch 380/400\n26/26 - train_loss: 0.2261 - train_accuracy: 95.1280%                 - val_loss: 0.0570 - val_accuracy: 94.9495%\nEpoch 381/400\n26/26 - train_loss: 0.2371 - train_accuracy: 94.4410%                 - val_loss: 0.0570 - val_accuracy: 94.9495%\nEpoch 382/400\n26/26 - train_loss: 0.2300 - train_accuracy: 94.6908%                 - val_loss: 0.0567 - val_accuracy: 94.9495%\nEpoch 383/400\n26/26 - train_loss: 0.2296 - train_accuracy: 94.6908%                 - val_loss: 0.0562 - val_accuracy: 94.9495%\nEpoch 384/400\n26/26 - train_loss: 0.2267 - train_accuracy: 94.2536%                 - val_loss: 0.0558 - val_accuracy: 94.9495%\nEpoch 385/400\n26/26 - train_loss: 0.2379 - train_accuracy: 94.1287%                 - val_loss: 0.0552 - val_accuracy: 94.9495%\nEpoch 386/400\n26/26 - train_loss: 0.2324 - train_accuracy: 93.8788%                 - val_loss: 0.0565 - val_accuracy: 94.9495%\nEpoch 387/400\n26/26 - train_loss: 0.2267 - train_accuracy: 94.5659%                 - val_loss: 0.0561 - val_accuracy: 94.9495%\nEpoch 388/400\n26/26 - train_loss: 0.2448 - train_accuracy: 94.2536%                 - val_loss: 0.0554 - val_accuracy: 94.9495%\nEpoch 389/400\n26/26 - train_loss: 0.2237 - train_accuracy: 94.7533%                 - val_loss: 0.0514 - val_accuracy: 94.9495%\nEpoch 390/400\n26/26 - train_loss: 0.2179 - train_accuracy: 95.0656%                 - val_loss: 0.0513 - val_accuracy: 94.9495%\nEpoch 391/400\n26/26 - train_loss: 0.2192 - train_accuracy: 94.8157%                 - val_loss: 0.0514 - val_accuracy: 94.9495%\nEpoch 392/400\n26/26 - train_loss: 0.2197 - train_accuracy: 95.0656%                 - val_loss: 0.0507 - val_accuracy: 94.9495%\nEpoch 393/400\n26/26 - train_loss: 0.2288 - train_accuracy: 94.3785%                 - val_loss: 0.0509 - val_accuracy: 94.9495%\nEpoch 394/400\n26/26 - train_loss: 0.2186 - train_accuracy: 94.8782%                 - val_loss: 0.0509 - val_accuracy: 94.9495%\nEpoch 395/400\n26/26 - train_loss: 0.2203 - train_accuracy: 94.3161%                 - val_loss: 0.0511 - val_accuracy: 94.9495%\nEpoch 396/400\n26/26 - train_loss: 0.2188 - train_accuracy: 94.5034%                 - val_loss: 0.0520 - val_accuracy: 94.9495%\nEpoch 397/400\n26/26 - train_loss: 0.2088 - train_accuracy: 95.2530%                 - val_loss: 0.0516 - val_accuracy: 94.9495%\nEpoch 398/400\n26/26 - train_loss: 0.2162 - train_accuracy: 94.6908%                 - val_loss: 0.0509 - val_accuracy: 94.9495%\nEpoch 399/400\n26/26 - train_loss: 0.2085 - train_accuracy: 95.4403%                 - val_loss: 0.0505 - val_accuracy: 94.9495%\nEpoch 400/400\n26/26 - train_loss: 0.2121 - train_accuracy: 95.5653%                 - val_loss: 0.0509 - val_accuracy: 95.4545%\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save\n",
        "model_save_path = \"model.pth\"\n",
        "\n",
        "torch.save({\n",
        "    'classifier_state_dict': prefix_tuning_model.classifier.state_dict(),\n",
        "    'prefix_embeddings': prefix_tuning_model.prefix_embeddings\n",
        "}, model_save_path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-29T04:14:27.154813Z",
          "iopub.execute_input": "2024-10-29T04:14:27.155528Z",
          "iopub.status.idle": "2024-10-29T04:14:27.161252Z",
          "shell.execute_reply.started": "2024-10-29T04:14:27.155491Z",
          "shell.execute_reply": "2024-10-29T04:14:27.160289Z"
        },
        "trusted": true,
        "id": "D-aHKpxVkM9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download from my github\n",
        "url = \"https://github.com/TheAlchemist010/DeepLearning-Notebooks/raw/refs/heads/main/FIT3181/model.pth\"\n",
        "response = requests.get(url)\n",
        "with open(\"downloaded_prefix_tuning.pth\", \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "checkpoint = torch.load(\"downloaded_prefix_tuning.pth\")\n",
        "\n",
        "dowloaded_model = PrefixTuningForClassification(\n",
        "    model_name=\"bert-base-uncased\",\n",
        "    prefix_length=20,\n",
        "    data_manager=dm\n",
        ").to(device)\n",
        "\n",
        "dowloaded_model.classifier.load_state_dict(checkpoint['classifier_state_dict'])\n",
        "dowloaded_model.prefix_embeddings = checkpoint['prefix_embeddings']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-29T04:16:53.887239Z",
          "iopub.execute_input": "2024-10-29T04:16:53.887884Z",
          "iopub.status.idle": "2024-10-29T04:16:54.609556Z",
          "shell.execute_reply.started": "2024-10-29T04:16:53.887843Z",
          "shell.execute_reply": "2024-10-29T04:16:54.608767Z"
        },
        "trusted": true,
        "id": "_C3QAGgrkM9c",
        "outputId": "30ad7764-dfef-4bce-d422-216920adab9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_30/962287828.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(\"downloaded_prefix_tuning.pth\")\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test on training set\n",
        "trainer = FineTunedBaseTrainer(model= dowloaded_model, criterion=criterion, optimizer=optimizer, train_loader=train_loader, val_loader=valid_loader)\n",
        "val_loss, val_accuracy = trainer.evaluate(trainer.val_loader)\n",
        "print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy*100:.2f}%')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-29T04:17:14.648119Z",
          "iopub.execute_input": "2024-10-29T04:17:14.648490Z",
          "iopub.status.idle": "2024-10-29T04:17:15.273065Z",
          "shell.execute_reply.started": "2024-10-29T04:17:14.648456Z",
          "shell.execute_reply": "2024-10-29T04:17:15.272052Z"
        },
        "trusted": true,
        "id": "XkprEmKikM9c",
        "outputId": "517b19b8-2308-469c-e973-46a84195156c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Validation Loss: 0.0165, Validation Accuracy: 97.47%\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<div style=\"text-align: center\"> <font color=\"green\">GOOD LUCK WITH YOUR ASSIGNMENT 2!</font> </div>\n",
        "<div style=\"text-align: center\"> <font color=\"black\">END OF ASSIGNMENT</font> </div>"
      ],
      "metadata": {
        "id": "evYeSuXm-OkM"
      }
    }
  ]
}