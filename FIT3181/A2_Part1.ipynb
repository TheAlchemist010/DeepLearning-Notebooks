{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheAlchemist010/DeepLearning-Notebooks/blob/main/FIT3181/A2_Part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"#0b486b\">  FIT3181: Deep Learning (2024) - Assignment 2 (Main Part)</font>\n",
        "***\n",
        "*CE/Lecturer (Clayton):*  **Dr Trung Le** | trunglm@monash.edu <br/>\n",
        "*Lecturer (Clayton):* **Prof Dinh Phung** | dinh.phung@monash.edu <br/>\n",
        "*Lecturer (Malaysia):*  **Dr Arghya Pal** | arghya.pal@monash.edu <br/>\n",
        "*Lecturer (Malaysia):*  **Dr Lim Chern Hong** | lim.chernhong@monash.edu <br/>  <br/>\n",
        "*Head Tutor 3181:*  **Miss Vy Vo** |  \\[v.vo@monash.edu \\] <br/>\n",
        "*Head Tutor 5215:*  **Dr Van Nguyen** |  \\[van.nguyen1@monash.edu \\]\n",
        "\n",
        "<br/> <br/>\n",
        "Faculty of Information Technology, Monash University, Australia\n",
        "***"
      ],
      "metadata": {
        "id": "qEHyseHxA8q4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"#0b486b\">  Student Information</font>\n",
        "***\n",
        "Surname: **Gallagher**  <br/>\n",
        "Firstname: **Daniel**    <br/>\n",
        "Student ID: **33094969**    <br/>\n",
        "Email: **dgal0013@student.monash.edu**    <br/>\n",
        "Your tutorial time: **Monday 4pm**    <br/>\n",
        "***"
      ],
      "metadata": {
        "id": "QqMi8gdDBD1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"0b486b\">Assignment 2 â€“ Deep Learning for Sequential Data</font>\n",
        "### Due: <font color=\"red\">11:55pm Sunday, 27 October 2024</font> (FIT3181)\n",
        "\n",
        "#### <font color=\"red\">Important note:</font> This is an **individual** assignment. It contributes **15%** to your final mark. Read the assignment instructions carefully."
      ],
      "metadata": {
        "id": "7e8EwkVtGva7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"#0b486b\">Assignment 2's Organization</font>\n",
        "This assignment 2 has two (2) sections:\n",
        "- Section 1: Fundamentals of RNNs (10 marks).\n",
        "- Section 2: Deep Learning for Sequential Data (90 marks). This section is further divided into 4 parts.\n",
        "\n",
        "The assignment 2 is organized in three (3) notebooks.\n",
        "- Notebook 1 (this notebook) [Total: 30 marks] includes Section 1 as well as Part 1 and Part 2 of Section 2.\n",
        "- Notebook 2 ([link](https://colab.research.google.com/drive/19-WnvLH24yUZ_eih3_8P_Fjn8cM8X2Gz?usp=sharing)) [Total: 40 marks] includes Part 3 of Section 2.\n",
        "- Notebook 3 ([link](https://colab.research.google.com/drive/1kZnS4MhgMkovuS2rij76tYrcBqr9xmE0?usp=sharing)) [Total: 30 marks] includes Part 4 of Section 2.\n"
      ],
      "metadata": {
        "id": "PF8vqRzTCEsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"#0b486b\">What to submit</font>\n",
        "\n",
        "This assignment is to be completed individually and submitted to Moodle unit site. **By the due date, you are required to submit one  <font color=\"red; font-weight:bold\">single zip file, named xxx_assignment02_solution.zip</font> where `xxx` is your student ID, to the corresponding Assignment (Dropbox) in Moodle**. You can use Google Colab to do Assignment 2 but you need to save it to an `*.ipynb` file to submit to the unit Moodle.\n",
        "\n",
        "**More importantly, if you use Google Colab to do this assignment, you need to first make a copy of this notebook on your Google drive**."
      ],
      "metadata": {
        "id": "WnlGo-hGBvlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***For example, if your student ID is <font color=\"red; font-weight:bold\">12356</font>, then gather all of your assignment solutions to a folder, create a zip file named <font color=\"red; font-weight:bold\">123456_assignment02_solution.zip</font> and submit this file.***"
      ],
      "metadata": {
        "id": "ZVGeGjkzn4RG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Within this zip folder, you **must** submit the following files <u>for each part</u>:\n",
        "1.\t**`FIT3181_DeepLearning_Assignment2_Official[Main].ipynb`**:  this is your Python notebook solution source file.\n",
        "1.\t**`FIT3181_DeepLearning_Assignment2_Official[Main].html`**: this is the output of your Python notebook solution *exported* in HTML format.\n",
        "1. **`FIT3181_DeepLearning_Assignment2_Official[RNNs].ipynb`**\n",
        "1. **`FIT3181_DeepLearning_Assignment2_Official[RNNs].html`**\n",
        "1. **`FIT3181_DeepLearning_Assignment2_Official[Transformers].ipynb`**\n",
        "1. **`FIT3181_DeepLearning_Assignment2_Official[Transformers].html`**\n",
        "1.\tAny **extra files or folder** needed to complete your assignment (e.g., images used in your answers).\n",
        "\n"
      ],
      "metadata": {
        "id": "u3J6gGAMn44Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1: Fundamentals in RNNs"
      ],
      "metadata": {
        "id": "LsB3OmMxB4Dh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need to **manually** implement a multi-timestep Recurrent Neural Network that can take an input as a 3D tensor `[batch_size, seq_len, input_size]` for a classification task.\n",
        "\n",
        "<div style=\"text-align: right\"><font color=\"red\">[Total: 10 marks]</font></div>"
      ],
      "metadata": {
        "id": "c83ea5R9nh_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "OzH6RTIjExy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We declare the relevant variables."
      ],
      "metadata": {
        "id": "yKlCx1D3rczS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 5\n",
        "seq_len = 4\n",
        "batch_size = 8\n",
        "hidden_size = 3\n",
        "num_classes = 3"
      ],
      "metadata": {
        "id": "C77pPiGBA9IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create random inputs (i.e., `inputs`) and random labels (i.e., `random_labels`)."
      ],
      "metadata": {
        "id": "A10tCeyKrkUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.randn(batch_size, seq_len, input_size)\n",
        "random_labels = torch.randint(0, num_classes, (batch_size,))\n",
        "print(inputs.shape)\n",
        "print(random_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybfqj-wBEu0H",
        "outputId": "403790b1-161c-496d-e695-b05280eef0a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 5])\n",
            "tensor([1, 2, 1, 2, 2, 2, 0, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) In what follows, we need to declare the model parameters, which include the matrices $U$ (``[input_size, hidden_size]``), W (``[hidden_size, hidden_size]``), $V$ (``[hidden_size, num_classes]``) and the biases $b$ and $c$ for the hidden states and logits respectively.\n",
        "\n",
        "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
      ],
      "metadata": {
        "id": "I7ABTPhyr6wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Insert your code here\n",
        "import torch.nn as nn\n",
        "\n",
        "U = torch.empty(input_size, hidden_size)\n",
        "W = torch.empty(hidden_size, hidden_size)\n",
        "b = torch.rand(hidden_size)\n",
        "V = torch.empty(hidden_size, num_classes)\n",
        "c = torch.rand(hidden_size)\n",
        "\n",
        "nn.init.xavier_uniform_(U, gain=nn.init.calculate_gain('relu'))\n",
        "nn.init.xavier_uniform_(W, gain=nn.init.calculate_gain('relu'))\n",
        "nn.init.xavier_uniform_(V, gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "U.requires_grad_()\n",
        "W.requires_grad_()\n",
        "V.requires_grad_()\n",
        "b.requires_grad_()\n",
        "c.requires_grad_()\n"
      ],
      "metadata": {
        "id": "qzlZtACzsbSR",
        "outputId": "15ef561b-3f09-4598-eb03-d19b423792f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6984, 0.5721, 0.0899], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) Next you need to write the code to compute `hiddens` which is a 3D tensor of the shape ``[batch_size, seq_len, hidden_size]`` using the formula of the simple/standard RNN cells. You can freely modify the code below.\n",
        "\n",
        "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
      ],
      "metadata": {
        "id": "MW_gal7Us9fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize hiddens for update.\n",
        "hiddens = torch.zeros(batch_size, seq_len, hidden_size)\n",
        "\n",
        "\n",
        "for t in range(seq_len):\n",
        "    x_t = inputs[:, t, :]\n",
        "\n",
        "    if t == 0:\n",
        "        h_t = torch.tanh(torch.matmul(x_t, U) + b)\n",
        "    else:\n",
        "        h_t = torch.tanh(torch.matmul(x_t, U) + torch.matmul(hiddens[:, t-1, :], W) + b)\n",
        "\n",
        "    hiddens = hiddens.clone()\n",
        "    hiddens[:, t, :] = h_t\n",
        "\n",
        "\n",
        "print(hiddens)"
      ],
      "metadata": {
        "id": "KwxqdXkTtaea",
        "outputId": "7423aeae-02be-497c-b2e6-b5ccd1412a50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.9864,  0.9884, -0.9934],\n",
            "         [-0.5937,  0.9720,  0.5537],\n",
            "         [ 0.9967,  0.9916, -0.0268],\n",
            "         [ 0.7547,  0.9933, -0.7609]],\n",
            "\n",
            "        [[ 0.7004,  0.1342, -0.3018],\n",
            "         [ 0.9255,  0.9284, -0.6409],\n",
            "         [ 0.7059,  0.5887,  0.1832],\n",
            "         [ 0.3598,  0.9459,  0.5200]],\n",
            "\n",
            "        [[-0.8841,  0.6788,  0.5224],\n",
            "         [-0.4491,  0.9997, -0.9854],\n",
            "         [ 0.4349, -0.2356,  0.8817],\n",
            "         [ 0.9865, -0.8166,  0.9763]],\n",
            "\n",
            "        [[ 1.0000,  0.6388,  0.8189],\n",
            "         [-0.6036,  0.4269,  0.5069],\n",
            "         [-0.8601,  0.9997, -0.9859],\n",
            "         [ 0.9988,  0.9536,  0.8209]],\n",
            "\n",
            "        [[ 0.4870,  0.1408,  0.1734],\n",
            "         [ 1.0000,  0.8074,  0.9354],\n",
            "         [-0.3557,  0.9909, -0.9217],\n",
            "         [ 0.9189,  0.4074,  0.9476]],\n",
            "\n",
            "        [[ 0.9873,  0.9997, -0.7413],\n",
            "         [ 0.4969,  0.8564,  0.7157],\n",
            "         [ 0.9994,  0.7622,  0.9616],\n",
            "         [ 0.9960,  0.9958, -0.7974]],\n",
            "\n",
            "        [[-0.9965,  0.2927, -0.5005],\n",
            "         [ 0.9997, -0.5986,  0.9972],\n",
            "         [ 0.5063,  0.8950, -0.7313],\n",
            "         [-0.9046,  0.9452, -0.7979]],\n",
            "\n",
            "        [[ 0.9731, -0.2530,  0.8792],\n",
            "         [ 0.5889, -0.9879,  0.9624],\n",
            "         [-0.2489, -0.9267,  0.7829],\n",
            "         [ 0.9999, -0.6399,  0.9966]]], grad_fn=<CopySlices>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(3) In what follows, you need to write the code to compute the logits based on the last hidden state (``[batch_size, hidden_size]``) of hiddens.\n",
        "\n",
        "<div style=\"text-align: right\"><font color=\"red\">[1 mark]</font></div>"
      ],
      "metadata": {
        "id": "sC3gaFvLtyUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = torch.matmul(hiddens[:, -1, :], V) + c\n",
        "print(logits)"
      ],
      "metadata": {
        "id": "Psr-vrw8uCMN",
        "outputId": "9466b7e8-331e-4458-8d41-4a59a17a5ab1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.5396,  1.5382, -2.1987],\n",
            "        [-0.6763,  0.1808, -0.6699],\n",
            "        [ 0.5563, -1.0177,  0.0588],\n",
            "        [-0.9486, -0.3149, -1.3148],\n",
            "        [-0.5639, -0.6015, -0.7289],\n",
            "        [ 0.5829,  1.5200, -2.5546],\n",
            "        [ 0.5504,  1.9794,  0.0539],\n",
            "        [ 0.3718, -0.9910, -0.0679]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(4) Write the code to compute the cross-entropy loss by comparing the logits to the labels. You can use PyTorch's built-in loss function.\n",
        "\n",
        "<div style=\"text-align: right\"><font color=\"red\">[1 mark]</font></div>"
      ],
      "metadata": {
        "id": "3DEJQQz9uKFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "loss = criterion(logits, random_labels)\n",
        "\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "Gyy7lbacuWPR",
        "outputId": "3c4d39b7-bb16-4f93-be48-917604a0c60c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.7577, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(5) Next, you need to do back-propagation to compute the gradients of the loss w.r.t. the model parameters. You can use PyTorch's built-in method to compute the gradients.\n",
        "\n",
        "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
      ],
      "metadata": {
        "id": "RVt-977bumET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "HSpeFroAu6oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(6) Finally, let assume that the learning rate $\\eta = 0.1$, you need to write the code to **manually** update the new model parameters using the SGD manner.\n",
        "\n",
        "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
      ],
      "metadata": {
        "id": "bEkHFzOovBx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "\n",
        "with torch.no_grad():\n",
        "    U -= learning_rate * U.grad\n",
        "    W -= learning_rate * W.grad\n",
        "    V -= learning_rate * V.grad\n",
        "    b -= learning_rate * b.grad\n",
        "    c -= learning_rate * c.grad\n",
        "\n",
        "U.grad.zero_()\n",
        "W.grad.zero_()\n",
        "V.grad.zero_()\n",
        "b.grad.zero_()\n",
        "c.grad.zero_()"
      ],
      "metadata": {
        "id": "2zFSfys8wMqs",
        "outputId": "8284270b-d8e6-4b68-fa8c-5a9198b8c864",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2: Deep Learning for Sequential Data"
      ],
      "metadata": {
        "id": "rAVPM0BnTdd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"#0b486b\">Set random seeds</font>"
      ],
      "metadata": {
        "id": "FAgUSME4TsCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start with importing PyTorch and NumPy and setting random seeds for PyTorch and NumPy. You can use any seeds you prefer."
      ],
      "metadata": {
        "id": "00nhh4IRUcGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import random\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import BertTokenizer\n",
        "import os\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')"
      ],
      "metadata": {
        "id": "O7XWUry0JXCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_all(seed=1029):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "seed_all(seed=1234)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "6ZoWqunmUY7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"#0b486b\">Download and preprocess the data</font>\n",
        "\n",
        "<div style=\"text-align: right\"><font color=\"red; font-weight:bold\"><span></div>"
      ],
      "metadata": {
        "id": "6VU1jS6SUl8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset we use for this assignment is a question classification dataset for which the training set consists of $5,500$ questions belonging to 6 coarse question categories including:\n",
        "- abbreviation (ABBR),\n",
        "- entity (ENTY),\n",
        "- description (DESC),\n",
        "- human (HUM),\n",
        "- location (LOC) and\n",
        "- numeric (NUM).\n",
        "\n",
        "In this assignment, we will utilize a subset of this dataset, containing $2,000$ questions for training and validation. We will use 80% of those 2000 questions for trainning and the rest for validation.\n"
      ],
      "metadata": {
        "id": "wQEzWmZjUulL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing data is a crucial initial step in any machine learning or deep learning project. The *TextDataManager* class simplifies the process by providing functionalities to download and preprocess data specifically designed for the subsequent questions in this assignment. It is highly recommended to gain a comprehensive understanding of the class's functionality by **carefully reading** the content provided in the *TextDataManager* class before proceeding to answer the questions."
      ],
      "metadata": {
        "id": "zOd49RTpUxxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataManager:\n",
        "    \"\"\"\n",
        "    This class manages and preprocesses a simple text dataset for a sentence classification task.\n",
        "\n",
        "    Attributes:\n",
        "        verbose (bool): Controls verbosity for printing information during data processing.\n",
        "        max_sentence_len (int): The maximum length of a sentence in the dataset.\n",
        "        str_questions (list): A list to store the string representations of the questions in the dataset.\n",
        "        str_labels (list): A list to store the string representations of the labels in the dataset.\n",
        "        numeral_labels (list): A list to store the numerical representations of the labels in the dataset.\n",
        "        numeral_data (list): A list to store the numerical representations of the questions in the dataset.\n",
        "        random_state (int): Seed value for random number generation to ensure reproducibility.\n",
        "            Set this value to a specific integer to reproduce the same random sequence every time. Defaults to 6789.\n",
        "        random (np.random.RandomState): Random number generator object initialized with the given random_state.\n",
        "            It is used for various random operations in the class.\n",
        "\n",
        "    Methods:\n",
        "        maybe_download(dir_name, file_name, url, verbose=True):\n",
        "            Downloads a file from a given URL if it does not exist in the specified directory.\n",
        "            The directory and file are created if they do not exist.\n",
        "\n",
        "        read_data(dir_name, file_names):\n",
        "            Reads data from files in a directory, preprocesses it, and computes the maximum sentence length.\n",
        "            Each file is expected to contain rows in the format \"<label>:<question>\".\n",
        "            The labels and questions are stored as string representations.\n",
        "\n",
        "        manipulate_data():\n",
        "            Performs data manipulation by tokenizing, numericalizing, and padding the text data.\n",
        "            The questions are tokenized and converted into numerical sequences using a tokenizer.\n",
        "            The sequences are padded or truncated to the maximum sequence length.\n",
        "\n",
        "        train_valid_test_split(train_ratio=0.9):\n",
        "            Splits the data into training, validation, and test sets based on a given ratio.\n",
        "            The data is randomly shuffled, and the specified ratio is used to determine the size of the training set.\n",
        "            The string questions, numerical data, and numerical labels are split accordingly.\n",
        "            TensorFlow `Dataset` objects are created for the training and validation sets.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, verbose=True, random_state=6789):\n",
        "        self.verbose = verbose\n",
        "        self.max_sentence_len = 0\n",
        "        self.str_questions = list()\n",
        "        self.str_labels = list()\n",
        "        self.numeral_labels = list()\n",
        "        self.maxlen = None\n",
        "        self.numeral_data = list()\n",
        "        self.random_state = random_state\n",
        "        self.random = np.random.RandomState(random_state)\n",
        "\n",
        "    @staticmethod\n",
        "    def maybe_download(dir_name, file_name, url, verbose=True):\n",
        "        if not os.path.exists(dir_name):\n",
        "            os.mkdir(dir_name)\n",
        "        if not os.path.exists(os.path.join(dir_name, file_name)):\n",
        "            urlretrieve(url + file_name, os.path.join(dir_name, file_name))\n",
        "        if verbose:\n",
        "            print(\"Downloaded successfully {}\".format(file_name))\n",
        "\n",
        "    def read_data(self, dir_name, file_names):\n",
        "        self.str_questions = list()\n",
        "        self.str_labels = list()\n",
        "        for file_name in file_names:\n",
        "            file_path= os.path.join(dir_name, file_name)\n",
        "            with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
        "                for row in f:\n",
        "                    row_str = row.split(\":\")\n",
        "                    label, question = row_str[0], row_str[1]\n",
        "                    question = question.lower()\n",
        "                    self.str_labels.append(label)\n",
        "                    self.str_questions.append(question[0:-1])\n",
        "                    if self.max_sentence_len < len(self.str_questions[-1]):\n",
        "                        self.max_sentence_len = len(self.str_questions[-1])\n",
        "\n",
        "        # turns labels into numbers\n",
        "        le = preprocessing.LabelEncoder()\n",
        "        le.fit(self.str_labels)\n",
        "        self.numeral_labels = np.array(le.transform(self.str_labels))\n",
        "        self.str_classes = le.classes_\n",
        "        self.num_classes = len(self.str_classes)\n",
        "        if self.verbose:\n",
        "            print(\"\\nSample questions and corresponding labels... \\n\")\n",
        "            print(self.str_questions[0:5])\n",
        "            print(self.str_labels[0:5])\n",
        "\n",
        "    def manipulate_data(self):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        vocab = self.tokenizer.get_vocab()\n",
        "        self.word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "        self.idx2word = {i:w for w,i in self.word2idx.items()}\n",
        "        self.vocab_size = len(self.word2idx)\n",
        "\n",
        "        token_ids = []\n",
        "        num_seqs = []\n",
        "        for text in self.str_questions:  # iterate over the list of text\n",
        "          text_seqs = self.tokenizer.tokenize(str(text))  # tokenize each text individually\n",
        "          # Convert tokens to IDs\n",
        "          token_ids = self.tokenizer.convert_tokens_to_ids(text_seqs)\n",
        "          # Convert token IDs to a tensor of indices using your word2idx mapping\n",
        "          seq_tensor = torch.LongTensor(token_ids)\n",
        "          num_seqs.append(seq_tensor)  # append the tensor for each sequence\n",
        "\n",
        "        # Pad the sequences and create a tensor\n",
        "        if num_seqs:\n",
        "          self.numeral_data = pad_sequence(num_seqs, batch_first=True)  # Pads to max length of the sequences\n",
        "          self.num_sentences, self.maxlen = self.numeral_data.shape\n",
        "\n",
        "    def train_valid_test_split(self, train_ratio=0.8, test_ratio = 0.1):\n",
        "        train_size = int(self.num_sentences*train_ratio) +1\n",
        "        test_size = int(self.num_sentences*test_ratio) +1\n",
        "        valid_size = self.num_sentences - (train_size + test_size)\n",
        "        data_indices = list(range(self.num_sentences))\n",
        "        random.shuffle(data_indices)\n",
        "        self.train_str_questions = [self.str_questions[i] for i in data_indices[:train_size]]\n",
        "        self.train_numeral_labels = self.numeral_labels[data_indices[:train_size]]\n",
        "        train_set_data = self.numeral_data[data_indices[:train_size]]\n",
        "        train_set_labels = self.numeral_labels[data_indices[:train_size]]\n",
        "        train_set_labels = torch.from_numpy(train_set_labels)\n",
        "        train_set = torch.utils.data.TensorDataset(train_set_data, train_set_labels)\n",
        "        self.test_str_questions = [self.str_questions[i] for i in data_indices[-test_size:]]\n",
        "        self.test_numeral_labels = self.numeral_labels[data_indices[-test_size:]]\n",
        "        test_set_data = self.numeral_data[data_indices[-test_size:]]\n",
        "        test_set_labels = self.numeral_labels[data_indices[-test_size:]]\n",
        "        test_set_labels = torch.from_numpy(test_set_labels)\n",
        "        test_set = torch.utils.data.TensorDataset(test_set_data, test_set_labels)\n",
        "        self.valid_str_questions = [self.str_questions[i] for i in data_indices[train_size:-test_size]]\n",
        "        self.valid_numeral_labels = self.numeral_labels[data_indices[train_size:-test_size]]\n",
        "        valid_set_data = self.numeral_data[data_indices[train_size:-test_size]]\n",
        "        valid_set_labels = self.numeral_labels[data_indices[train_size:-test_size]]\n",
        "        valid_set_labels = torch.from_numpy(valid_set_labels)\n",
        "        valid_set = torch.utils.data.TensorDataset(valid_set_data, valid_set_labels)\n",
        "        self.train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "        self.test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
        "        self.valid_loader = DataLoader(valid_set, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "_C2fuJNzUhha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Loading data...')\n",
        "DataManager.maybe_download(\"data\", \"train_2000.label\", \"http://cogcomp.org/Data/QA/QC/\")\n",
        "\n",
        "dm = DataManager()\n",
        "dm.read_data(\"data/\", [\"train_2000.label\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3npdESj6Vb_t",
        "outputId": "d56b4ae5-4573-428d-8953-7e2e67cf88df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Downloaded successfully train_2000.label\n",
            "\n",
            "Sample questions and corresponding labels... \n",
            "\n",
            "['manner how did serfdom develop in and then leave russia ?', 'cremat what films featured the character popeye doyle ?', \"manner how can i find a list of celebrities ' real names ?\", 'animal what fowl grabs the spotlight after the chinese year of the monkey ?', 'exp what is the full form of .com ?']\n",
            "['DESC', 'ENTY', 'DESC', 'ENTY', 'ABBR']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dm.manipulate_data()\n",
        "dm.train_valid_test_split(train_ratio=0.8, test_ratio = 0.1)"
      ],
      "metadata": {
        "id": "EgrYZPmyVj60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dm.train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH-U0sUMVnW-",
        "outputId": "323671a2-b6b5-4625-9a88-ffff1ea1187b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 36]) torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"#0b486b\">Part 1: Using Word2Vect to transform texts to vectors </font>\n",
        "\n",
        "<div style=\"text-align: right\"><font color=\"red; font-weight:bold\">[Total marks for this part: 10 marks]<span></div>"
      ],
      "metadata": {
        "id": "frFf7-ehVvNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "KZFrETlMVq7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"red\">**Question 1.1**</font>\n",
        "**Write code to download the pretrained model *glove-wiki-gigaword-100*. Note that this model transforms a word in its dictionary to a $100$ dimensional vector.**\n",
        "\n",
        "**Write code for the function *get_word_vector(word, model)* used to transform a word to a vector using the pretrained Word2Vect model *model*. Note that for a word not in the vocabulary of our *word2vect*, you need to return a vector $0$ with 100 dimensions.**\n",
        "\n",
        "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
      ],
      "metadata": {
        "id": "4JH-f1BJY9bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2vect = api.load(\"glove-wiki-gigaword-100\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b76lh_hZVyac",
        "outputId": "753493a8-d20c-46b8-8efd-6894e92d3419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_vector(word, model):\n",
        "    try:\n",
        "        vector = model[word]\n",
        "    except:\n",
        "        vector = np.zeros(100)\n",
        "    return vector"
      ],
      "metadata": {
        "id": "GpbibKfdZB-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"red\">**Question 1.2**</font>\n",
        "\n",
        "**Write the code for the function `get_sentence_vector(sentence, important_score=None, model= None)`. Note that this function will transform a sentence to a 100-dimensional vector using the pretrained model *model*. In addition, the list *important_score* which has the same length as the *sentence* specifies the important scores of the words in the sentence. In your code, you first need to apply *softmax* function over *important_score* to obtain the important weight *important_weight* which forms a probability over the words of the sentence. Furthermore, the final vector of the sentence will be weighted sum of the individual vectors for words and the weights in *important_weight*.**\n",
        "- $important\\_weight = softmax(important\\_score)$.\n",
        "- $final\\_vector= important\\_weight[1]\\times v[1] + important\\_weight[2]\\times v[2] + ...+ important\\_weight[T]\\times v[T]$ where $T$ is the length of the sentence and $v[i]$ is the vector representation of the $i-th$  word in this sentence.\n",
        "\n",
        "**Note that if `important_score=None` is set by default, your function should return the average of all representation vectors corresponding to set `important_score=[1,1,...,1]`.**\n",
        "\n",
        "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
      ],
      "metadata": {
        "id": "8TjRdyTg2_dN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_vector(sentence, important_score=None, model= None):\n",
        "    words = sentence.split()\n",
        "\n",
        "    if important_score is None:\n",
        "      important_score = [1.] * len(words)\n",
        "\n",
        "    important_score = torch.softmax(torch.tensor(important_score), dim=0)\n",
        "\n",
        "    vector = np.zeros(100)\n",
        "    for i, word in enumerate(words):\n",
        "        vector += get_word_vector(word, model) * important_score[i].item()\n",
        "\n",
        "    return vector\n"
      ],
      "metadata": {
        "id": "MaPs4Hqa2_30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"red\">**Question 1.3**</font>\n",
        "\n",
        "**Write code to transform questions in *dm.train_str_questions* and *dm.valid_str_questions* to feature vectors. Note that after running the following cells, you must have $X\\_train$ and $X\\_valid$ which are two NumPy arrays of the feature vectors and $y\\_train$ and $y\\_valid$ which are two arrays of numeric labels (Hint: *dm.train_numeral_labels* and *dm.valid_numeral_labels*). You can add more lines to the following cells if necessary. In addition, you should decide the *important_score* by yourself. For example, the 1st score is 1, the 2nd score is decayed by 0.9, the 3rd is decayed by 0.9, and so on.**\n",
        "\n",
        "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
      ],
      "metadata": {
        "id": "Tu63GJ2c3IgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "longest_sentence_len = max(len(sentence.split()) for sentence in dm.train_str_questions)\n",
        "important_score = [pow(0.9, i) for i in range(longest_sentence_len)]"
      ],
      "metadata": {
        "id": "rQJKCaInWGNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Transform training set to feature vectors...\")\n",
        "X_train = [get_sentence_vector(sentence, model=word2vect, important_score=important_score) for sentence in dm.train_str_questions]\n",
        "y_train = dm.train_numeral_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SCsWIdZ3I4S",
        "outputId": "bb9f0765-03e8-49f4-c623-dc08f80f84e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transform training set to feature vectors...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Transform validation set to feature vectors...\")\n",
        "X_valid = [get_sentence_vector(sentence, model=word2vect, important_score=important_score) for sentence in dm.valid_str_questions]\n",
        "y_valid = dm.valid_numeral_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2odznptZ3Nf_",
        "outputId": "397474cb-630b-4fdd-c800-9c35939e5ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transform validation set to feature vectors...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"red\">**Question 1.4**</font>\n",
        "\n",
        "**It is now to use *MinMaxScaler(feature_range=(-1,1))* in scikit-learn to scale both training and validation sets to the range $(-1,1)$.**\n",
        "\n",
        "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
      ],
      "metadata": {
        "id": "gOp_gy3d3Reh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)"
      ],
      "metadata": {
        "id": "eT47Q13c3VJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"red\">**Question 1.5**</font>\n",
        "**Train a Logistic Regression model on the training set and then evaluate on the validation set.** You can use any classification metrics in `sklearn` for evaluation.\n",
        "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
      ],
      "metadata": {
        "id": "NcvKQyHO3ZJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "IQYE_rz-3b25",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "y_pred = model.predict(X_valid)\n",
        "accuracy = metrics.accuracy_score(y_valid, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "ucJWNHMO3gXF",
        "outputId": "2301f88d-c43c-429b-b601-4995b14ec76e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9090909090909091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now declare the `BaseTrainer` class, which will be used later to train the subsequent deep learning models for text data."
      ],
      "metadata": {
        "id": "NZs9Y0rl7gBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class BaseTrainer:\n",
        "    def __init__(self, model, criterion, optimizer, train_loader, val_loader):\n",
        "        self.model = model\n",
        "        self.criterion = criterion  #the loss function\n",
        "        self.optimizer = optimizer  #the optimizer\n",
        "        self.train_loader = train_loader  #the train loader\n",
        "        self.val_loader = val_loader  #the valid loader\n",
        "\n",
        "    #the function to train the model in many epochs\n",
        "    def fit(self, num_epochs):\n",
        "        self.num_batches = len(self.train_loader)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "            train_loss, train_accuracy = self.train_one_epoch()\n",
        "            val_loss, val_accuracy = self.validate_one_epoch()\n",
        "            print(\n",
        "                f'{self.num_batches}/{self.num_batches} - train_loss: {train_loss:.4f} - train_accuracy: {train_accuracy*100:.4f}% \\\n",
        "                - val_loss: {val_loss:.4f} - val_accuracy: {val_accuracy*100:.4f}%')\n",
        "\n",
        "    #train in one epoch, return the train_acc, train_loss\n",
        "    def train_one_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        train_accuracy = correct / total\n",
        "        train_loss = running_loss / self.num_batches\n",
        "        return train_loss, train_accuracy\n",
        "\n",
        "    #evaluate on a loader and return the loss and accuracy\n",
        "    def evaluate(self, loader):\n",
        "        self.model.eval()\n",
        "        loss, correct, total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for data in loader:\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = correct / total\n",
        "        loss = loss / len(self.val_loader)\n",
        "        return loss, accuracy\n",
        "\n",
        "    #return the val_acc, val_loss, be called at the end of each epoch\n",
        "    def validate_one_epoch(self):\n",
        "      val_loss, val_accuracy = self.evaluate(self.val_loader)\n",
        "      return val_loss, val_accuracy"
      ],
      "metadata": {
        "id": "yXlNQvGn7OEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"#0b486b\">Part 2: Text CNN for sequence modeling and neural embedding </font>\n",
        "\n",
        "<div style=\"text-align: right\"><font color=\"red; font-weight:bold\">[Total marks for this part: 10 marks]<span></div>"
      ],
      "metadata": {
        "id": "3GWOQN_S7p4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In what follows, you are required to complete the code for Text CNN for sentence classification. The paper of Text CNN can be found at this [link](https://www.aclweb.org/anthology/D14-1181.pdf). Here is the description of the Text CNN that you need to construct.**\n",
        "- There are three attributes (properties or instance variables): *embed_size, state_size, data_manager*.\n",
        "  - `embed_size`: the dimension of the vector space for which the words are embedded to using the embedding matrix.\n",
        "  - `state_size`: the number of filters used in *Conv1D* (reference [here](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)).\n",
        "  - `data_manager`: the data manager to store information of the dataset.\n",
        "- The detail of the computational process is as follows:\n",
        "  - Given input $x$, we embed $x$ using the embedding matrix to obtain an $3D$ tensor $[batch\\_size, seq\\_len, embed\\_size]$ as $e$.\n",
        "  - We feed $e$ to three *Conv1D* layers, each of which has $state\\_size$ filters, activation= $relu$, and $kernel\\_size= 3, 5, 7$ respectively to obtain $h1, h2, h3$. Note that each $h1, h2, h3$ is a 3D tensor with the shape $[batch\\_size, state\\_size, output\\_size]$. Moreover, you need to apply *Conv1D* to the $seq\\_len$ dimension.\n",
        "  - We then apply *GlobalMaxPool1D()* (reference [here](https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool1d.html#torch.nn.functional.max_pool1d)) over $h1, h2, h3$ to obtain 2D tensors stored in $h1, h2, h3$ again.\n",
        "  - We then concatenate three 2D tensors $h1, h2, h3$ to obtain $h$ with the shape $\\left[batch\\_size, 3\\times state\\_size\\right]$. Note that you need to specify the axis to concatenate.\n",
        "  - We finally build up one dense layer $\\left[3\\times state\\_size, num\\_classes\\right]$  on the top of $h$ for classification.\n",
        "  "
      ],
      "metadata": {
        "id": "QJub3Fwm7vC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TextCNN(torch.nn.Module):\n",
        "    def __init__(self, embed_size= 128, state_size=16, data_manager=None):\n",
        "        super().__init__()\n",
        "        self.data_manager = data_manager\n",
        "        self.embed_size = embed_size\n",
        "        self.state_size = state_size\n",
        "        self.embed = nn.Embedding(self.data_manager.vocab_size, self.embed_size)\n",
        "        self.conv1d_1 = nn.Conv1d(in_channels=self.embed_size, out_channels=self.state_size, kernel_size=3)\n",
        "        self.conv1d_2 = nn.Conv1d(in_channels=self.embed_size, out_channels=self.state_size, kernel_size=5)\n",
        "        self.conv1d_3 = nn.Conv1d(in_channels=self.embed_size, out_channels=self.state_size, kernel_size=7)\n",
        "        self.fc = nn.Linear(state_size*3, self.data_manager.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e = self.embed(x)\n",
        "        #permute x before applying Conv1D\n",
        "        e= e.permute(0,2,1)\n",
        "\n",
        "        #applying Conv1D\n",
        "        h1 = self.conv1d_1(e)\n",
        "        h2 = self.conv1d_2(e)\n",
        "        h3 = self.conv1d_3(e)\n",
        "\n",
        "        #apply relu\n",
        "        h1 = F.relu(h1)\n",
        "        h2 = F.relu(h2)\n",
        "        h3 = F.relu(h3)\n",
        "\n",
        "        #apply GlobalMaxPool\n",
        "        h1 = F.max_pool1d(h1, h1.size(2)).squeeze(2)\n",
        "        h2 = F.max_pool1d(h2, h2.size(2)).squeeze(2)\n",
        "        h3 = F.max_pool1d(h3, h3.size(2)).squeeze(2)\n",
        "\n",
        "\n",
        "\n",
        "        h = torch.cat((h1, h2, h3), dim=1)\n",
        "        h = self.fc(h)\n",
        "        return h"
      ],
      "metadata": {
        "id": "lEnG-BGJ239k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We declare `text_cnn` and train on several epochs (e.g., `50 epochs`).\n"
      ],
      "metadata": {
        "id": "n-PN6KmIFw6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_cnn = TextCNN(data_manager=dm).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(text_cnn.parameters(), lr=0.001)\n",
        "trainer = BaseTrainer(model=text_cnn, criterion=criterion, optimizer=optimizer, train_loader=dm.train_loader, val_loader=dm.valid_loader)\n",
        "trainer.fit(num_epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrL5oM82BR3_",
        "outputId": "763413e7-8866-4164-c794-23551399f4fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "26/26 - train_loss: 1.4556 - train_accuracy: 45.4716%                 - val_loss: 0.5070 - val_accuracy: 74.7475%\n",
            "Epoch 2/50\n",
            "26/26 - train_loss: 0.8497 - train_accuracy: 78.2636%                 - val_loss: 0.2092 - val_accuracy: 84.3434%\n",
            "Epoch 3/50\n",
            "26/26 - train_loss: 0.4888 - train_accuracy: 87.2580%                 - val_loss: 0.1547 - val_accuracy: 90.4040%\n",
            "Epoch 4/50\n",
            "26/26 - train_loss: 0.2969 - train_accuracy: 94.1911%                 - val_loss: 0.0696 - val_accuracy: 93.4343%\n",
            "Epoch 5/50\n",
            "26/26 - train_loss: 0.1901 - train_accuracy: 96.6271%                 - val_loss: 0.0554 - val_accuracy: 94.9495%\n",
            "Epoch 6/50\n",
            "26/26 - train_loss: 0.1339 - train_accuracy: 97.9388%                 - val_loss: 0.0329 - val_accuracy: 94.9495%\n",
            "Epoch 7/50\n",
            "26/26 - train_loss: 0.0918 - train_accuracy: 99.2505%                 - val_loss: 0.0215 - val_accuracy: 94.9495%\n",
            "Epoch 8/50\n",
            "26/26 - train_loss: 0.0693 - train_accuracy: 99.4379%                 - val_loss: 0.0173 - val_accuracy: 94.9495%\n",
            "Epoch 9/50\n",
            "26/26 - train_loss: 0.0510 - train_accuracy: 99.9375%                 - val_loss: 0.0108 - val_accuracy: 95.4545%\n",
            "Epoch 10/50\n",
            "26/26 - train_loss: 0.0393 - train_accuracy: 100.0000%                 - val_loss: 0.0094 - val_accuracy: 95.4545%\n",
            "Epoch 11/50\n",
            "26/26 - train_loss: 0.0318 - train_accuracy: 100.0000%                 - val_loss: 0.0078 - val_accuracy: 95.4545%\n",
            "Epoch 12/50\n",
            "26/26 - train_loss: 0.0281 - train_accuracy: 100.0000%                 - val_loss: 0.0060 - val_accuracy: 95.4545%\n",
            "Epoch 13/50\n",
            "26/26 - train_loss: 0.0214 - train_accuracy: 100.0000%                 - val_loss: 0.0052 - val_accuracy: 95.9596%\n",
            "Epoch 14/50\n",
            "26/26 - train_loss: 0.0171 - train_accuracy: 100.0000%                 - val_loss: 0.0038 - val_accuracy: 95.9596%\n",
            "Epoch 15/50\n",
            "26/26 - train_loss: 0.0142 - train_accuracy: 100.0000%                 - val_loss: 0.0032 - val_accuracy: 95.9596%\n",
            "Epoch 16/50\n",
            "26/26 - train_loss: 0.0121 - train_accuracy: 100.0000%                 - val_loss: 0.0030 - val_accuracy: 95.9596%\n",
            "Epoch 17/50\n",
            "26/26 - train_loss: 0.0117 - train_accuracy: 100.0000%                 - val_loss: 0.0026 - val_accuracy: 95.9596%\n",
            "Epoch 18/50\n",
            "26/26 - train_loss: 0.0103 - train_accuracy: 100.0000%                 - val_loss: 0.0021 - val_accuracy: 96.4646%\n",
            "Epoch 19/50\n",
            "26/26 - train_loss: 0.0083 - train_accuracy: 100.0000%                 - val_loss: 0.0019 - val_accuracy: 96.4646%\n",
            "Epoch 20/50\n",
            "26/26 - train_loss: 0.0071 - train_accuracy: 100.0000%                 - val_loss: 0.0017 - val_accuracy: 95.9596%\n",
            "Epoch 21/50\n",
            "26/26 - train_loss: 0.0063 - train_accuracy: 100.0000%                 - val_loss: 0.0015 - val_accuracy: 96.4646%\n",
            "Epoch 22/50\n",
            "26/26 - train_loss: 0.0056 - train_accuracy: 100.0000%                 - val_loss: 0.0014 - val_accuracy: 95.9596%\n",
            "Epoch 23/50\n",
            "26/26 - train_loss: 0.0051 - train_accuracy: 100.0000%                 - val_loss: 0.0012 - val_accuracy: 95.9596%\n",
            "Epoch 24/50\n",
            "26/26 - train_loss: 0.0046 - train_accuracy: 100.0000%                 - val_loss: 0.0011 - val_accuracy: 95.9596%\n",
            "Epoch 25/50\n",
            "26/26 - train_loss: 0.0042 - train_accuracy: 100.0000%                 - val_loss: 0.0010 - val_accuracy: 96.4646%\n",
            "Epoch 26/50\n",
            "26/26 - train_loss: 0.0038 - train_accuracy: 100.0000%                 - val_loss: 0.0010 - val_accuracy: 96.4646%\n",
            "Epoch 27/50\n",
            "26/26 - train_loss: 0.0036 - train_accuracy: 100.0000%                 - val_loss: 0.0009 - val_accuracy: 96.4646%\n",
            "Epoch 28/50\n",
            "26/26 - train_loss: 0.0032 - train_accuracy: 100.0000%                 - val_loss: 0.0008 - val_accuracy: 96.4646%\n",
            "Epoch 29/50\n",
            "26/26 - train_loss: 0.0030 - train_accuracy: 100.0000%                 - val_loss: 0.0008 - val_accuracy: 96.4646%\n",
            "Epoch 30/50\n",
            "26/26 - train_loss: 0.0028 - train_accuracy: 100.0000%                 - val_loss: 0.0007 - val_accuracy: 96.4646%\n",
            "Epoch 31/50\n",
            "26/26 - train_loss: 0.0030 - train_accuracy: 100.0000%                 - val_loss: 0.0007 - val_accuracy: 96.4646%\n",
            "Epoch 32/50\n",
            "26/26 - train_loss: 0.0025 - train_accuracy: 100.0000%                 - val_loss: 0.0007 - val_accuracy: 96.4646%\n",
            "Epoch 33/50\n",
            "26/26 - train_loss: 0.0024 - train_accuracy: 100.0000%                 - val_loss: 0.0006 - val_accuracy: 96.4646%\n",
            "Epoch 34/50\n",
            "26/26 - train_loss: 0.0021 - train_accuracy: 100.0000%                 - val_loss: 0.0005 - val_accuracy: 96.4646%\n",
            "Epoch 35/50\n",
            "26/26 - train_loss: 0.0020 - train_accuracy: 100.0000%                 - val_loss: 0.0005 - val_accuracy: 96.4646%\n",
            "Epoch 36/50\n",
            "26/26 - train_loss: 0.0018 - train_accuracy: 100.0000%                 - val_loss: 0.0005 - val_accuracy: 96.4646%\n",
            "Epoch 37/50\n",
            "26/26 - train_loss: 0.0018 - train_accuracy: 100.0000%                 - val_loss: 0.0005 - val_accuracy: 96.4646%\n",
            "Epoch 38/50\n",
            "26/26 - train_loss: 0.0017 - train_accuracy: 100.0000%                 - val_loss: 0.0004 - val_accuracy: 96.4646%\n",
            "Epoch 39/50\n",
            "26/26 - train_loss: 0.0016 - train_accuracy: 100.0000%                 - val_loss: 0.0004 - val_accuracy: 96.4646%\n",
            "Epoch 40/50\n",
            "26/26 - train_loss: 0.0015 - train_accuracy: 100.0000%                 - val_loss: 0.0004 - val_accuracy: 96.4646%\n",
            "Epoch 41/50\n",
            "26/26 - train_loss: 0.0020 - train_accuracy: 100.0000%                 - val_loss: 0.0004 - val_accuracy: 96.4646%\n",
            "Epoch 42/50\n",
            "26/26 - train_loss: 0.0016 - train_accuracy: 100.0000%                 - val_loss: 0.0003 - val_accuracy: 96.4646%\n",
            "Epoch 43/50\n",
            "26/26 - train_loss: 0.0013 - train_accuracy: 100.0000%                 - val_loss: 0.0003 - val_accuracy: 96.4646%\n",
            "Epoch 44/50\n",
            "26/26 - train_loss: 0.0012 - train_accuracy: 100.0000%                 - val_loss: 0.0003 - val_accuracy: 96.4646%\n",
            "Epoch 45/50\n",
            "26/26 - train_loss: 0.0012 - train_accuracy: 100.0000%                 - val_loss: 0.0003 - val_accuracy: 96.4646%\n",
            "Epoch 46/50\n",
            "26/26 - train_loss: 0.0011 - train_accuracy: 100.0000%                 - val_loss: 0.0003 - val_accuracy: 96.4646%\n",
            "Epoch 47/50\n",
            "26/26 - train_loss: 0.0010 - train_accuracy: 100.0000%                 - val_loss: 0.0003 - val_accuracy: 96.4646%\n",
            "Epoch 48/50\n",
            "26/26 - train_loss: 0.0010 - train_accuracy: 100.0000%                 - val_loss: 0.0002 - val_accuracy: 96.4646%\n",
            "Epoch 49/50\n",
            "26/26 - train_loss: 0.0013 - train_accuracy: 100.0000%                 - val_loss: 0.0002 - val_accuracy: 96.4646%\n",
            "Epoch 50/50\n",
            "26/26 - train_loss: 0.0011 - train_accuracy: 100.0000%                 - val_loss: 0.0002 - val_accuracy: 96.4646%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We evaluate the trained model on the testing set."
      ],
      "metadata": {
        "id": "yrjO5iT6F_Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = trainer.evaluate(dm.test_loader)\n",
        "print(f'test_loss: {test_loss:.4f} - test_accuracy: {test_acc*100:.4f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnHiWCl7BtJC",
        "outputId": "b97191f1-2834-4bd5-ba3b-10d982f55e5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_loss: 0.2111 - test_accuracy: 95.5224%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to html A2_Part3.ipynb"
      ],
      "metadata": {
        "id": "L3M_-Zi9ipzG",
        "outputId": "c433dac2-0342-43bc-935a-5fa4efe3218b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook A2_Part3.ipynb to html\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/jupyter-nbconvert\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jupyter_core/application.py\", line 283, in launch_instance\n",
            "    super().launch_instance(argv=argv, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nbconvert/nbconvertapp.py\", line 423, in start\n",
            "    self.convert_notebooks()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nbconvert/nbconvertapp.py\", line 597, in convert_notebooks\n",
            "    self.convert_single_notebook(notebook_filename)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nbconvert/nbconvertapp.py\", line 560, in convert_single_notebook\n",
            "    output, resources = self.export_single_notebook(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nbconvert/nbconvertapp.py\", line 488, in export_single_notebook\n",
            "    output, resources = self.exporter.from_filename(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nbconvert/exporters/exporter.py\", line 189, in from_filename\n",
            "    return self.from_file(f, resources=resources, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nbconvert/exporters/exporter.py\", line 206, in from_file\n",
            "    return self.from_notebook_node(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nbconvert/exporters/html.py\", line 223, in from_notebook_node\n",
            "    return super().from_notebook_node(nb, resources, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nbconvert/exporters/templateexporter.py\", line 413, in from_notebook_node\n",
            "    output = self.template.render(nb=nb_copy, resources=resources)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jinja2/environment.py\", line 1304, in render\n",
            "    self.environment.handle_exception()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jinja2/environment.py\", line 939, in handle_exception\n",
            "    raise rewrite_traceback_stack(source=source)\n",
            "  File \"/usr/local/share/jupyter/nbconvert/templates/lab/index.html.j2\", line 3, in top-level template code\n",
            "    {% from 'jupyter_widgets.html.j2' import jupyter_widgets %}\n",
            "  File \"/usr/local/share/jupyter/nbconvert/templates/lab/base.html.j2\", line 2, in top-level template code\n",
            "    {% from 'celltags.j2' import celltags %}\n",
            "  File \"/usr/local/share/jupyter/nbconvert/templates/base/display_priority.j2\", line 1, in top-level template code\n",
            "    {%- extends 'base/null.j2' -%}\n",
            "  File \"/usr/local/share/jupyter/nbconvert/templates/base/null.j2\", line 26, in top-level template code\n",
            "    {%- block body -%}\n",
            "  File \"/usr/local/share/jupyter/nbconvert/templates/base/null.j2\", line 29, in block 'body'\n",
            "    {%- block body_loop -%}\n",
            "  File \"/usr/local/share/jupyter/nbconvert/templates/base/null.j2\", line 31, in block 'body_loop'\n",
            "    {%- block any_cell scoped -%}\n",
            "  File \"/usr/local/share/jupyter/nbconvert/templates/base/null.j2\", line 34, in block 'any_cell'\n",
            "    {%- block codecell scoped -%}\n",
            "  File \"/usr/local/share/jupyter/nbconvert/templates/lab/base.html.j2\", line 12, in block 'codecell'\n",
            "    {{ super() }}\n",
            "  File \"/usr/local/share/jupyter/nbconvert/templates/base/null.j2\", line 44, in block 'codecell'\n",
            "    {%- block output_group -%}\n",
            "  File \"/usr/local/share/jupyter/nbconvert/templates/lab/base.html.j2\", line 38, in block 'output_group'\n",
            "    {{ super() }}\n",
            "  File \"/usr/local/share/jupyter/nbconvert/templates/base/null.j2\", line 48, in block 'output_group'\n",
            "    {%- block outputs scoped -%}\n",
            "  File \"/usr/local/share/jupyter/nbconvert/templates/lab/base.html.j2\", line 44, in block 'outputs'\n",
            "    {{ super() }}\n",
            "  File \"/usr/local/share/jupyter/nbconvert/templates/base/null.j2\", line 50, in block 'outputs'\n",
            "    {%- block output scoped -%}\n",
            "  File \"/usr/local/share/jupyter/nbconvert/templates/lab/base.html.j2\", line 87, in block 'output'\n",
            "    {{ super() }}\n",
            "  File \"/usr/local/share/jupyter/nbconvert/templates/base/null.j2\", line 67, in block 'output'\n",
            "    {%- block display_data scoped -%}\n",
            "  File \"/usr/local/share/jupyter/nbconvert/templates/base/null.j2\", line 68, in block 'display_data'\n",
            "    {%- block data_priority scoped -%}\n",
            "  File \"/usr/local/share/jupyter/nbconvert/templates/lab/base.html.j2\", line 126, in block 'data_priority'\n",
            "    {{ super() }}\n",
            "  File \"/usr/local/share/jupyter/nbconvert/templates/base/display_priority.j2\", line 7, in block 'data_priority'\n",
            "    {%- for type in output.data | filter_data_type -%}\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nbconvert/filters/widgetsdatatypefilter.py\", line 57, in __call__\n",
            "    metadata[\"widgets\"][WIDGET_STATE_MIMETYPE][\"state\"]\n",
            "KeyError: 'state'\n"
          ]
        }
      ]
    }
  ]
}